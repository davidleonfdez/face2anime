{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:07.138936Z",
     "iopub.status.busy": "2021-06-11T23:14:07.138466Z",
     "iopub.status.idle": "2021-06-11T23:14:10.384279Z",
     "shell.execute_reply": "2021-06-11T23:14:10.382858Z",
     "shell.execute_reply.started": "2021-06-11T23:14:07.138874Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from functools import partial\n",
    "import gc\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "from sys import platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:10.386926Z",
     "iopub.status.busy": "2021-06-11T23:14:10.386550Z",
     "iopub.status.idle": "2021-06-11T23:14:10.391524Z",
     "shell.execute_reply": "2021-06-11T23:14:10.390055Z",
     "shell.execute_reply.started": "2021-06-11T23:14:10.386872Z"
    }
   },
   "outputs": [],
   "source": [
    "run_as_standalone_nb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:10.393808Z",
     "iopub.status.busy": "2021-06-11T23:14:10.393480Z",
     "iopub.status.idle": "2021-06-11T23:14:10.408551Z",
     "shell.execute_reply": "2021-06-11T23:14:10.407043Z",
     "shell.execute_reply.started": "2021-06-11T23:14:10.393775Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_as_standalone_nb:\n",
    "    root_lib_path = Path('face2anime').resolve()\n",
    "    if not root_lib_path.exists():\n",
    "        !git clone https://github.com/davidleonfdez/face2anime.git\n",
    "    if str(root_lib_path) not in sys.path:\n",
    "        sys.path.insert(0, str(root_lib_path))\n",
    "else:\n",
    "    import local_lib_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:10.410843Z",
     "iopub.status.busy": "2021-06-11T23:14:10.410384Z",
     "iopub.status.idle": "2021-06-11T23:14:10.448041Z",
     "shell.execute_reply": "2021-06-11T23:14:10.447137Z",
     "shell.execute_reply.started": "2021-06-11T23:14:10.410807Z"
    }
   },
   "outputs": [],
   "source": [
    "from face2anime.gen_utils import is_iterable\n",
    "from face2anime.layers import (ConcatPoolHalfDownsamplingOp2d, ConvHalfDownsamplingOp2d, FeatureStatType, \n",
    "                               TransformsLayer, ParentNetSource, ResBlockDown)\n",
    "from face2anime.losses import (ContentLossCallback, CritPredsTracker, CycleConsistencyLossCallback, \n",
    "                               CrossIdentityLossCallback, CycleGANLoss, IdentityLossCallback, LossWrapper, \n",
    "                               MultiCritPredsTracker, R1GANGPCallback)\n",
    "from face2anime.misc import FeaturesCalculator\n",
    "from face2anime.networks import (CycleCritic, CycleGenerator, default_decoder, default_encoder, \n",
    "                                 Img2ImgGenerator, PatchResCritic, res_critic)\n",
    "from face2anime.plot import plot_c_preds, plot_multi_c_preds\n",
    "from face2anime.train_utils import (add_ema_to_gan_learner, clean_mem, custom_load_model,\n",
    "                                    custom_save_model, EpochFilterAll, SaveCheckpointsCallback)\n",
    "from face2anime.transforms import AdaptiveAugmentsCallback, ADATransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:18.158561Z",
     "iopub.status.busy": "2021-06-11T23:14:18.158022Z",
     "iopub.status.idle": "2021-06-11T23:14:18.164549Z",
     "shell.execute_reply": "2021-06-11T23:14:18.163722Z",
     "shell.execute_reply.started": "2021-06-11T23:14:18.158519Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "img_size = 64\n",
    "n_channels = 3\n",
    "bs = 64\n",
    "save_cycle_len = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to use two different datasets, with each of them containing images from one specific domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain A ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input_a_fns` must be set to a list that contains the paths of the training images that belong to domain A. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:21.850121Z",
     "iopub.status.busy": "2021-06-11T23:14:21.849672Z",
     "iopub.status.idle": "2021-06-11T23:14:26.108131Z",
     "shell.execute_reply": "2021-06-11T23:14:26.106803Z",
     "shell.execute_reply.started": "2021-06-11T23:14:21.850081Z"
    }
   },
   "outputs": [],
   "source": [
    "celeba_path = Path('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba')\n",
    "#input_fns = get_image_files(celeba_path)\n",
    "# get_image_files is too slow, there's no need to check the extension here\n",
    "input_a_fns = celeba_path.ls()\n",
    "input_a_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain B ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `anime_ds_path` must be set to the path of the parent folder of the training images that belong to domain B. \n",
    "\n",
    "In this example, the ds used is \"animecharacterfaces\", by Kaggle user *aadilmalik94*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:21.319844Z",
     "iopub.status.busy": "2021-06-11T23:14:21.319150Z",
     "iopub.status.idle": "2021-06-11T23:14:21.331206Z",
     "shell.execute_reply": "2021-06-11T23:14:21.329732Z",
     "shell.execute_reply.started": "2021-06-11T23:14:21.319790Z"
    }
   },
   "outputs": [],
   "source": [
    "anime_ds_path = Path('/kaggle/input/animecharacterfaces/animeface-character-dataset/data').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ds path passed to `dblock.dataloaders()` or `ImageDataLoaders.from_dblock()` will be forwarded\n",
    "to `get_items`, which will return a list of items, usually a list of image paths if `get_items=get_image_files`.\n",
    "\n",
    "So, for each item, we are expected to receive a filename `fn` and be able to\n",
    "derive x and y from it, with `get_x(fn)` and `get_y(fn)`.\n",
    "\n",
    "For bidirectional unpaired image to image translation, we can:\n",
    "* Use the domain B ds path as the DataBlock `source`. \n",
    "* Load independently the filenames of the domain A ds; let's call it `input_a_fns`\n",
    "* `get_y` needs two functions:\n",
    "  * The first one can just return the path received (domain B).\n",
    "  * The second returns a random item from `input_a_fns` (domain A).\n",
    "* `get_x` also needs two functions:\n",
    "  * The first one return a random item from `input_a_fns` (domain A).\n",
    "  * The second can just return the path received (domain B).\n",
    "* `get_x` and `get_y` are called every time a data item is used; so, by using random, we can be sure every x is not tied to a fixed y; i.e., they won't be together in the same (x, y) batch every epoch for loss calculation.\n",
    "* In this case, domain A: human faces; domain B: anime faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:30.218622Z",
     "iopub.status.busy": "2021-06-11T23:14:30.218152Z",
     "iopub.status.idle": "2021-06-11T23:14:35.438287Z",
     "shell.execute_reply": "2021-06-11T23:14:35.436844Z",
     "shell.execute_reply.started": "2021-06-11T23:14:30.218581Z"
    }
   },
   "outputs": [],
   "source": [
    "class Domain(Enum):\n",
    "    A = 0\n",
    "    B = 1\n",
    "\n",
    "\n",
    "def reverse_domain(domain): return Domain.B if domain == Domain.A else Domain.A\n",
    "\n",
    "\n",
    "def get_random_fn_a(fn):\n",
    "    return input_a_fns[random.randint(0, len(input_a_fns)-1)]\n",
    "\n",
    "\n",
    "def get_input_dependant_fn_a(fn):\n",
    "    return input_a_fns[hash(fn) % len(input_a_fns)]\n",
    "\n",
    "\n",
    "normalize_tf = Normalize.from_stats(torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5]))\n",
    "\n",
    "\n",
    "def get_dblock(extra_batch_tfms=None):\n",
    "    if extra_batch_tfms is None: extra_batch_tfms = []    \n",
    "    return DataBlock(blocks=(ImageBlock, ImageBlock, ImageBlock, ImageBlock),\n",
    "                     get_x=[get_random_fn_a, noop],\n",
    "                     get_y=[noop, get_random_fn_a],\n",
    "                     get_items=get_image_files,\n",
    "                     #get_items=lambda path: target_fns,\n",
    "                     splitter=IndexSplitter([]),\n",
    "                     item_tfms=Resize(img_size, method=ResizeMethod.Crop), \n",
    "                     batch_tfms=[normalize_tf] + extra_batch_tfms,\n",
    "                     n_inp=2)\n",
    "\n",
    "\n",
    "dblock = get_dblock()\n",
    "main_path = anime_ds_path\n",
    "dls = dblock.dataloaders(main_path, path=main_path, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:14:35.449272Z",
     "iopub.status.busy": "2021-06-11T23:14:35.448785Z",
     "iopub.status.idle": "2021-06-11T23:14:36.993156Z",
     "shell.execute_reply": "2021-06-11T23:14:36.991842Z",
     "shell.execute_reply.started": "2021-06-11T23:14:35.449233Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_batch = dls.one_batch()\n",
    "titles = ['x1 (A)', 'x2 (B)', 'y1 (B)', 'y2 (A)']\n",
    "_, axs = plt.subplots(1, 4)\n",
    "for t, ax, title in zip(sample_batch, axs, titles):\n",
    "    normalize_tf.decode(t)[0].show(ax=ax, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:15:08.710511Z",
     "iopub.status.busy": "2021-06-11T23:15:08.710021Z",
     "iopub.status.idle": "2021-06-11T23:15:08.730701Z",
     "shell.execute_reply": "2021-06-11T23:15:08.729335Z",
     "shell.execute_reply.started": "2021-06-11T23:15:08.710474Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_n(learner, inputs_idxs:Union[int, Tuple[int, int]], max_bs=64):\n",
    "    dummy_path = Path('.')\n",
    "    items = learner.dls.train.items\n",
    "    if isinstance(inputs_idxs, int):\n",
    "        n_imgs = inputs_idxs\n",
    "        ini_idx = 0\n",
    "        end_idx = n_imgs\n",
    "    else:\n",
    "        ini_idx, end_idx = inputs_idxs\n",
    "        n_imgs = end_idx - ini_idx\n",
    "    if len(items) < end_idx: items = list(itertools.islice(itertools.cycle(items), ini_idx, end_idx))\n",
    "    dl = learner.dls.test_dl(items[:n_imgs], bs=max_bs)   \n",
    "    inp, _, _, dec_imgs_t = learner.get_preds(dl=dl, with_input=True, with_decoded=True)\n",
    "    dec_batch = dls.decode_batch(inp + dec_imgs_t, max_n=n_imgs)\n",
    "    return dec_batch\n",
    "\n",
    "\n",
    "def predict_show_n(learner, n_imgs, **predict_n_kwargs):\n",
    "    preds_batch = predict_n(learner, n_imgs, **predict_n_kwargs)\n",
    "    _, axs = plt.subplots(n_imgs, 4, figsize=(12, n_imgs * 3))\n",
    "    for i, (in_a, in_b, pred_a2b, pred_b2a) in enumerate(preds_batch):\n",
    "        in_a.show(ax=axs[i][0], title='In A')\n",
    "        pred_a2b.show(ax=axs[i][1], title='Out A->B')\n",
    "        in_b.show(ax=axs[i][2], title='In B')\n",
    "        pred_b2a.show(ax=axs[i][3], title='Out B->A')\n",
    "        \n",
    "        \n",
    "def save_preds(c_preds_tracker, filepaths):\n",
    "    return [df.to_csv(filepath) for df, filepath in zip(c_preds_tracker.to_dfs(), filepaths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-11T23:15:08.947733Z",
     "iopub.status.busy": "2021-06-11T23:15:08.947176Z",
     "iopub.status.idle": "2021-06-11T23:15:08.972059Z",
     "shell.execute_reply": "2021-06-11T23:15:08.970196Z",
     "shell.execute_reply.started": "2021-06-11T23:15:08.947691Z"
    }
   },
   "outputs": [],
   "source": [
    "def _forward_batch(model, batch, device):\n",
    "    input = batch[:2]\n",
    "    if device is not None:\n",
    "        for i in range(2): input[i] = input[i].to(device)\n",
    "    model(*input)\n",
    "\n",
    "\n",
    "def create_learner(for_inference=False, dblock=dblock, dls=dls, gp_w=10., latent_sz=100, \n",
    "                   mid_mlp_depth=2, g_norm=NormType.Instance, n_crit_iters=3,\n",
    "                   n_extra_convs_by_c_res_block=1, cycle_cons_w=1., id_loss_w=1., \n",
    "                   cross_id_loss_w=0, use_patch_critic=True, ftrs_stats=FeatureStatType.MEAN, \n",
    "                   ftrs_stats_source=None):\n",
    "    leakyReLU02 = partial(nn.LeakyReLU, negative_slope=0.2)\n",
    "    down_op = ConvHalfDownsamplingOp2d(ks=4, act_cls=leakyReLU02, bn_1st=False,\n",
    "                                       norm_type=NormType.Batch)\n",
    "    id_down_op = ConcatPoolHalfDownsamplingOp2d(conv_ks=3, act_cls=None, norm_type=None)\n",
    "    crit_args = [img_size, n_channels, down_op, id_down_op]\n",
    "    if use_patch_critic: crit_args.insert(2, img_size//8)\n",
    "    crit_kwargs = dict(n_extra_convs_by_res_block=n_extra_convs_by_c_res_block, \n",
    "                       act_cls=leakyReLU02, bn_1st=False, n_features=128, \n",
    "                       flatten_full=True)\n",
    "    if use_patch_critic:\n",
    "        crit_kwargs['ftrs_stats'] = ftrs_stats\n",
    "        crit_kwargs['ftrs_stats_source'] = ftrs_stats_source\n",
    "        crit_kwargs['input_norm_tf'] = normalize_tf\n",
    "        crit_kwargs['device'] = device\n",
    "    crit_builder = PatchResCritic if use_patch_critic else res_critic  \n",
    "    base_critics = [crit_builder(*crit_args, **crit_kwargs) for _ in range(2)]\n",
    "    base_critics = CycleCritic(*base_critics)\n",
    "    critic = base_critics\n",
    "    \n",
    "    def _decoder_builder(imsz, nch, latsz, hooks_by_sz=None): \n",
    "        return default_decoder(imsz, nch, latsz, norm_type=g_norm, hooks_by_sz=hooks_by_sz)\n",
    "    generators = [Img2ImgGenerator(img_size, n_channels, mid_mlp_depth=mid_mlp_depth, skip_connect=True,\n",
    "                                   encoder=default_encoder(img_size, n_channels, latent_sz, norm_type=g_norm),\n",
    "                                   decoder_builder=_decoder_builder)\n",
    "                  for _ in range(2)]\n",
    "    generator = CycleGenerator(*generators)\n",
    "    \n",
    "    cbs = []\n",
    "    c_loss_interceptors = []\n",
    "    metrics = []\n",
    "    if not for_inference:\n",
    "        # Pass base_critic to avoid grid_sample 2nd order derivative issue with ada critic\n",
    "        cbs.append(R1GANGPCallback(weight=gp_w, critic=base_critics))\n",
    "        if cycle_cons_w > 0: \n",
    "            cbs.append(CycleConsistencyLossCallback(generator.g_a2b, \n",
    "                                                    generator.g_b2a, \n",
    "                                                    weight=cycle_cons_w))\n",
    "            metrics.append('cycle_loss')\n",
    "        if id_loss_w > 0:\n",
    "            cbs.append(IdentityLossCallback(generator.g_a2b, \n",
    "                                            generator.g_b2a, \n",
    "                                            weight=id_loss_w))\n",
    "            metrics.append('identity_loss')\n",
    "        if cross_id_loss_w > 0:\n",
    "            cbs.append(CrossIdentityLossCallback(generator.g_a2b, \n",
    "                                                 generator.g_b2a, \n",
    "                                                 weight=cross_id_loss_w))\n",
    "            metrics.append('cross_identity_loss')\n",
    "        overall_crit_preds_tracker = MultiCritPredsTracker(reduce_batch=True)\n",
    "        c_loss_interceptors.append(overall_crit_preds_tracker)\n",
    "        \n",
    "    def gen_loss_func(*args): return 0\n",
    "    crit_loss_func = nn.BCEWithLogitsLoss()\n",
    "    loss_G, loss_C = gan_loss_from_func(gen_loss_func, crit_loss_func)\n",
    "    loss_C = LossWrapper(loss_C, c_loss_interceptors)\n",
    "    \n",
    "    learn = GANLearner(dls, generator, critic, loss_G, loss_C,\n",
    "                       opt_func=partial(Adam, mom=0., sqr_mom=0.99, wd=0.),\n",
    "                       cbs=cbs, switcher=FixedGANSwitcher(n_crit=n_crit_iters, n_gen=1),\n",
    "                       switch_eval=False, metrics=LossMetrics(metrics) or None)\n",
    "    learn.loss_func = CycleGANLoss(learn.loss_func)\n",
    "    learn.recorder.train_metrics=True\n",
    "    learn.recorder.valid_metrics=False\n",
    "    add_ema_to_gan_learner(learn, dblock, decay=0.999, forward_batch=_forward_batch)\n",
    "    if not for_inference: learn.crit_preds_tracker = overall_crit_preds_tracker\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating an instance of a fastai GANLearner object with the help of our custom `create_learner` method.\n",
    "\n",
    "The main parameters you may need to tweak are:\n",
    "\n",
    "* `use_patch_critic`: if False, a global critic is used instead of a patch critic. A global critic could be preferable when there's not a clear pixel correspondence between an image from one domain and its expected translation from the other domain.\n",
    "* `gp_w`: gradient penalty strength.\n",
    "* `g_norm`: type of normalization performed by the generator. The most common choice is NormType.Instance but NormType.Batch could produce even better results for EMA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner_1(*args, **kwargs):\n",
    "    return create_learner(*args, **kwargs, g_norm=NormType.Batch)\n",
    "\n",
    "learn = create_learner_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative model (Pytorch's nn.Module) that contains an EMA of the weights of the generator is stored in `learn.ema_model` and updated after every optimizer step. To handle it in an easier way, we should a create a learner for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_g_learn = Learner(dls, learn.ema_model, loss_func=lambda *args: torch.tensor(0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will (almost always) generate higher quality images than the trained generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to automatically store checkpoints of your model in the middle of a training run, it can be done with `SaveCheckpointsCallback`. The parameter `save_cycle_len` lets you choose the number of epochs between checkpoints.\n",
    "\n",
    "Given that the EMA model is not trained, the running statistics of its BN layers need to be updated manually after training. This update is actually done automatically by an `EMACallback` (attached by `create_learner`), only at the end of each `learn.fit()` call because it's expensive. However, as a consequence, the checkpoints saved during a single `fit` execution will have outdated BN running stats; to avoid this situation, you can pass an aditional parameter `pre_save_actions = [UpdateEMAPreSaveAction(learn, epoch_filter=EpochFilterAll())]` to enforce than the aforementioned update is performed right before the creation of every checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_save_actions = [UpdateEMAPreSaveAction(learn, epoch_filter=EpochFilterAll())]\n",
    "learn.add_cb(SaveCheckpointsCallback('face2anime_bidir_tr1', initial_epoch=1,\n",
    "                                     save_cycle_len=save_cycle_len,\n",
    "                                     pre_save_actions=pre_save_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If \"before the creation of every checkpoint\" is too often for your needs, a different epoch filter can be used:\n",
    "* `EpochFilterMultipleOfN`\n",
    "* `EpochFilterAfterN`\n",
    "* `ComposedEpochFilter`\n",
    "\n",
    "For instance, if `epoch_filter = EpochFilterMultipleOfN(3)`, the BN update of the EMA generator will only be executed before saving the checkpoints at epochs that are common multiples of `save_cycle_len` and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to choose a learning rate. At least at the beginning, anything between [1e-4, 5e-4] should be reasonable. We then call `learner.fit` with a number of epochs and the lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "learn.fit(200, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect a sample of results, one should call `predict_show_n` with the learner and the number of images to show. Before, we need to temporarily disable some callbacks only needed for training that would otherwise be called when obtaining the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_to_remove_for_display = [learn.save_checkpoints, \n",
    "                             learn.cycle_consistency_loss, \n",
    "                             learn.identity_loss]\n",
    "with learn.removed_cbs(cbs_to_remove_for_display) as displayable_learn:\n",
    "    predict_show_n(displayable_learn, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show some images produced by the EMA generator, we just need to call `predict_show_n` with `ema_g_learn` as its first parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_show_n(ema_g_learn, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learner also tracks a history of the logits of the predictions output by each of the two critics in the attribute `crit_preds_tracker`. It actually stores the mean by batch, not every prediction. To plot them, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi_c_preds(learn.crit_preds_tracker, ['A->B', 'B->A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you desire to preserve the aforementioned history, you should call `save_preds` and provide two paths with .csv extension: the first one for the critic of the transformation A->B (i.e. the critic of domain B) and the second for the critic of the transformation B->A (i.e. the critic of domain A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(learn.crit_preds_tracker, \n",
    "           [Path('crit_preds_face2anime_bidir_a2b_tr1_200ep.csv'),\n",
    "            Path('crit_preds_face2anime_bidir_b2a_tr1_200ep.csv')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resume a training run from a past session, first of all, we create the learners as always:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner(g_norm=NormType.Batch)\n",
    "ema_g_learn = Learner(dls, learn.ema_model, loss_func=lambda *args: torch.tensor(0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When attaching `SaveCheckpointsCallback`, remember to set `initial_epoch` to the numbers of epochs already completed plus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.add_cb(SaveCheckpointsCallback('face2anime_bidir_tr1', initial_epoch=201,\n",
    "                                     save_cycle_len=save_cycle_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to load a saved model, you need at least two files:\n",
    "* A .pth file containing the weights of the trained generator.\n",
    "* A .pth file containing the weights of the EMA generator.\n",
    "\n",
    "If you want to continue tracking the history of critic predictions, so that the new ones are appended to the old history, you must also pass the two .csv files indicated in the previous section.\n",
    "\n",
    "`custom_load_model` assumes the same naming convention than `custom_save_model`, which is the method used internally by `SaveCheckpointsCallback`, so only one filename is required for the two pytorch files. \n",
    "\n",
    "For instance, the example in the following cell assumes these locations of the input files:\n",
    "\n",
    "* ../input/face2anime-bidir/face2anime_bidir_tr1_200ep.pth\n",
    "* ../input/face2anime-bidir/face2anime_bidir_tr1_200ep_ema.pth\n",
    "* ./crit_preds_face2anime_bidir_b2a_tr1_200ep.csv\n",
    "* ./crit_preds_face2anime_bidir_a2b_tr1_200ep.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_load_model(learn, 'face2anime_bidir_tr1_200ep', base_path='../input/face2anime-bidir/', with_ema=True)\n",
    "preds_df_a = pd.read_csv(Path('crit_preds_face2anime_bidir_b2a_tr1_200ep.csv'), index_col=0)\n",
    "preds_df_b = pd.read_csv(Path('crit_preds_face2anime_bidir_a2b_tr1_200ep.csv'), index_col=0)        \n",
    "learn.crit_preds_tracker.load_from_dfs([preds_df_b, preds_df_a], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can go on exactly like in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our trained models, we are going to use FID as metric. We'll rely on an external python package called pytorch-fid. This package just needs the paths of two directories as inputs, one for each set of images to be compared; so, before invoking the FID process, we need to store in disk a set of real images and a set of fake images generated by the model we are evaluating.\n",
    "\n",
    "Most papers use 50000 images for each set, but 10000 images should be enough for a relative comparison between our own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:30:11.754568Z",
     "iopub.status.busy": "2021-06-09T12:30:11.754307Z",
     "iopub.status.idle": "2021-06-09T12:30:11.773534Z",
     "shell.execute_reply": "2021-06-09T12:30:11.77263Z",
     "shell.execute_reply.started": "2021-06-09T12:30:11.754541Z"
    }
   },
   "outputs": [],
   "source": [
    "base_fid_samples_path = Path('./fid_samples')\n",
    "n_fid_imgs = 10000\n",
    "\n",
    "\n",
    "def download_pytorch_fid_calculator():        \n",
    "    if platform == 'win32':\n",
    "        # As of 08/21, installing from PyPI on Windows doesn't work\n",
    "        !pip install git+https://github.com/mseitzer/pytorch-fid.git\n",
    "    else:\n",
    "        !pip install pytorch-fid\n",
    "\n",
    "    \n",
    "def create_fid_dirs(base_fid_samples_path):\n",
    "    base_fid_samples_path.mkdir()\n",
    "    (base_fid_samples_path/'fake').mkdir()\n",
    "    (base_fid_samples_path/'fake/A').mkdir()\n",
    "    (base_fid_samples_path/'fake/B').mkdir()\n",
    "    (base_fid_samples_path/'real').mkdir()\n",
    "    (base_fid_samples_path/'real/A').mkdir()\n",
    "    (base_fid_samples_path/'real/B').mkdir()\n",
    "    (base_fid_samples_path/'input').mkdir()\n",
    "    (base_fid_samples_path/'input/A').mkdir()\n",
    "    (base_fid_samples_path/'input/B').mkdir()\n",
    "\n",
    "    \n",
    "def save_real_imgs(dls, base_path, n_imgs=10000):\n",
    "    n_imgs_left = n_imgs\n",
    "    while n_imgs_left > 0:\n",
    "        b = dls.one_batch()\n",
    "        bs = b[1].size()[0]\n",
    "        dec_b = dls.decode_batch(b, max_n=bs)\n",
    "        for i in range(bs):\n",
    "            if n_imgs_left == 0: break\n",
    "            in_a_t, in_b_t, _, _ = dec_b[i]\n",
    "            img_a = PILImage.create(in_a_t)\n",
    "            img_b = PILImage.create(in_b_t)\n",
    "            img_idx = n_imgs_left-1\n",
    "            img_a.save(base_path/f'real/A/{img_idx}.jpg')\n",
    "            img_b.save(base_path/f'real/B/{img_idx}.jpg')\n",
    "            #if n_imgs_left % 1000 == 0: print(\"saved \" + str(img_idx))\n",
    "            n_imgs_left -= 1    \n",
    "\n",
    "            \n",
    "def save_fake_imgs(learner, base_path, n_imgs=10000, max_pred_sz=5000, save_inputs=False, \n",
    "                   **predict_n_kwargs):\n",
    "    n_imgs_left = n_imgs\n",
    "    n_chunks = math.ceil(n_imgs/max_pred_sz)\n",
    "    idxs = [(i*max_pred_sz, min(n_imgs, (i+1)*max_pred_sz)) for i in range(n_chunks)]\n",
    "    for ini_idx, end_idx in idxs:\n",
    "        preds_batch = predict_n(learner, (ini_idx, end_idx), **predict_n_kwargs)\n",
    "        for i, (in_a, in_b, img_t_a2b, img_t_b2a) in enumerate(preds_batch):\n",
    "            idx = i + ini_idx\n",
    "            PILImage.create(img_t_a2b).save(base_path/f'fake/B/{idx}.jpg')\n",
    "            PILImage.create(img_t_b2a).save(base_path/f'fake/A/{idx}.jpg')\n",
    "            if save_inputs:\n",
    "                PILImage.create(in_a).save(base_path/f'input/A/{idx}.jpg')\n",
    "                PILImage.create(in_b).save(base_path/f'input/B/{idx}.jpg')\n",
    "        preds_batch = None\n",
    "        clean_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must begin by setting up the environment:\n",
    "* Install pytorch-fid Python package\n",
    "* Create the directories where the output images will be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if run_as_standalone_nb:\n",
    "download_pytorch_fid_calculator()\n",
    "create_fid_dirs(base_fid_samples_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of real images from each domain must be saved into the corresponding directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:30:22.761492Z",
     "iopub.status.busy": "2021-06-09T12:30:22.761137Z",
     "iopub.status.idle": "2021-06-09T12:31:39.671585Z",
     "shell.execute_reply": "2021-06-09T12:31:39.670788Z",
     "shell.execute_reply.started": "2021-06-09T12:30:22.761465Z"
    }
   },
   "outputs": [],
   "source": [
    "save_real_imgs(dls, base_fid_samples_path, n_fid_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:30:22.743407Z",
     "iopub.status.busy": "2021-06-09T12:30:22.743034Z",
     "iopub.status.idle": "2021-06-09T12:30:22.759911Z",
     "shell.execute_reply": "2021-06-09T12:30:22.759063Z",
     "shell.execute_reply.started": "2021-06-09T12:30:22.743366Z"
    }
   },
   "outputs": [],
   "source": [
    "class FIDEvalType(Enum):\n",
    "    FAKE_VS_TARGET = 1\n",
    "    INPUT_VS_FAKE = 2\n",
    "\n",
    "\n",
    "def exec_fid_proc(domain, eval_type, base_fid_samples_path):\n",
    "    second_set_path = (base_fid_samples_path/'real'/domain.name if eval_type == FIDEvalType.FAKE_VS_TARGET \n",
    "                       else base_fid_samples_path/'input'/reverse_domain(domain).name)\n",
    "    return subprocess.run([\"python\", \"-m\", \"pytorch_fid\", base_fid_samples_path/'fake'/domain.name, \n",
    "                           second_set_path], \n",
    "                          stdout=subprocess.PIPE)\n",
    "\n",
    "\n",
    "def fid_out_to_arr(fid_proc_out):\n",
    "    if isinstance(fid_proc_out, bytes):\n",
    "        fid_proc_out = fid_proc_out.decode(sys.stdout.encoding)\n",
    "    # TODO: a regex could be more robust\n",
    "    float_fids = [round(float(line[5:].strip()), ndigits=1) \n",
    "                  for line in fid_proc_out.split('\\n') \n",
    "                  if line.startswith('FID')]\n",
    "    return float_fids\n",
    "\n",
    "\n",
    "def eval_models(builders, n_epochs, eval_type=FIDEvalType.FAKE_VS_TARGET, \n",
    "                base_fid_samples_path='./fid_samples', base_models_path='./models', \n",
    "                fn_suffix='', ema=False):\n",
    "    assert is_iterable(builders) or is_iterable(n_epochs)\n",
    "    if not is_iterable(builders): \n",
    "        builders = [builders] * len(list(n_epochs))\n",
    "    if not is_iterable(n_epochs): \n",
    "        n_epochs = [n_epochs] * len(list(builders))\n",
    "    result = {d.name: [] for d in Domain}\n",
    "    for builder, n_ep in zip(builders, n_epochs):\n",
    "        model_id = builder.__name__.split('_')[-1]\n",
    "        learner = builder(for_inference=True)\n",
    "        custom_load_model(learner, f'face2anime_bidir_tr{model_id}{fn_suffix}_{n_ep}ep', with_opt=False,\n",
    "                          base_path=base_models_path, with_ema=ema)\n",
    "        if ema: \n",
    "            learner = Learner(learner.dls, learner.ema_model,\n",
    "                              loss_func=lambda *args: torch.tensor(0.))\n",
    "        save_fake_imgs(learner, base_fid_samples_path, n_imgs=n_fid_imgs, \n",
    "                       save_inputs=(eval_type==FIDEvalType.INPUT_VS_FAKE))\n",
    "        for domain in Domain:\n",
    "            completed_proc = exec_fid_proc(domain, eval_type, base_fid_samples_path)\n",
    "            fid_value = fid_out_to_arr(completed_proc.stdout)\n",
    "            result[domain.name].extend(fid_value)\n",
    "            print(f'---- {model_id} ({domain.name}), after {n_ep} epochs ----')\n",
    "            print(completed_proc.stdout)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `eval_models` does everything else needed to get the FID measurements requested: it loads the models, generates the fake images, saves them and runs the FID evaluation process.\n",
    "\n",
    "Its parameters are:\n",
    "* `builders`: a list of methods that receive, at least, an optional parameter `for_inference` and return a learner.\n",
    "* `n_epochs`: an iterable containing the indexes of the epochs at which we want to evaluate the learners built by calling the methods passed as `builders`.\n",
    "* `eval_type`: the default is FIDEvalType.FAKE_VS_TARGET, which is the usual FID measurement that compares a set of generated images against a set of images of the target domain. If you pass FIDEvalType.INPUT_VS_FAKE, the set of input images is compared with the set of output images; it can serve as a measurement of content preservation.\n",
    "* `base_models_path`: path or str that points to the directory that contains all the .pth files.\n",
    "* `base_fid_samples_path`: path of the root folder of the fid samples directory tree. It must be same path passed to `create_fid_dirs` and `save_real_imgs`.\n",
    "* `fn_suffix`: substring expected to appear in the model filenames after the model id.\n",
    "* `ema`: indicates if the EMA generators must be used to create the fake images, instead of the trained generators.\n",
    "\n",
    "The filename of the .pth files is infered from the builder name. For instance, if we called `eval_models` with this combinations of parameters:\n",
    "\n",
    "```\n",
    "eval_models([create_learner_1, create_learner_2], range(5, 201, 5), fn_suffix='rerun', \n",
    "            base_models_path='./models', base_fid_samples_path=base_fid_samples_path)\n",
    "```\n",
    "\n",
    "the models files would be expected to be located in:\n",
    "\n",
    "* ./models/face2anime_bidir_tr1rerun_5ep.pth\n",
    "* ./models/face2anime_bidir_tr1rerun_5ep_ema.pth\n",
    "* ./models/face2anime_bidir_tr1rerun_10ep.pth\n",
    "* ./models/face2anime_bidir_tr1rerun_10ep_ema.pth\n",
    "* ...\n",
    "* ./models/face2anime_bidir_tr1rerun_200ep.pth\n",
    "* ./models/face2anime_bidir_tr1rerun_200ep_ema.pth\n",
    "* ./models/face2anime_bidir_tr2rerun_5ep.pth\n",
    "* ./models/face2anime_bidir_tr2rerun_5ep_ema.pth\n",
    "* ./models/face2anime_bidir_tr2rerun_10ep.pth\n",
    "* ./models/face2anime_bidir_tr2rerun_10ep_ema.pth\n",
    "* ...\n",
    "* ./models/face2anime_bidir_tr2rerun_200ep.pth\n",
    "* ./models/face2anime_bidir_tr2rerun_200ep_ema.pth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T17:33:05.364809Z",
     "iopub.status.busy": "2021-06-09T17:33:05.364456Z",
     "iopub.status.idle": "2021-06-09T17:56:56.772764Z",
     "shell.execute_reply": "2021-06-09T17:56:56.771877Z",
     "shell.execute_reply.started": "2021-06-09T17:33:05.364774Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_models(create_learner_1, range(5, 201, 5), fn_suffix='', base_models_path='./models',\n",
    "            base_fid_samples_path=base_fid_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T18:01:03.03276Z",
     "iopub.status.busy": "2021-06-09T18:01:03.03239Z",
     "iopub.status.idle": "2021-06-09T18:25:18.725024Z",
     "shell.execute_reply": "2021-06-09T18:25:18.724081Z",
     "shell.execute_reply.started": "2021-06-09T18:01:03.032727Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_models(create_learner_1, range(5, 201, 5), base_path='./models', fn_suffix='', ema=True,\n",
    "            base_fid_samples_path=base_fid_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models(create_learner_1, range(5, 201, 5), base_path='./models', fn_suffix='',\n",
    "            ema=True, eval_type=FIDEvalType.INPUT_VS_FAKE, \n",
    "            base_fid_samples_path=base_fid_samples_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to show some saved images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.create(base_fid_samples_path/'fake/A/10.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.create(base_fid_samples_path/'fake/B/10.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.create(base_fid_samples_path/'real/A/10.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.create(base_fid_samples_path/'real/B/10.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
