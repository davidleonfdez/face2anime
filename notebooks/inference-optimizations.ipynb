{"cells":[{"cell_type":"markdown","metadata":{},"source":["This notebook presents a variety of tools for automatic optimization of Pytorch modules for inference.\n","\n","You need at least the following requirements:\n","\n","- fastai>=2.3.1\n","- opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["run_as_standalone_nb = True\n","\n","\n","from pathlib import Path\n","import sys\n","\n","\n","if run_as_standalone_nb:\n","    root_lib_path = Path('face2anime').resolve()\n","    if not root_lib_path.exists():\n","        !git clone https://github.com/davidleonfdez/face2anime.git\n","    if str(root_lib_path) not in sys.path:\n","        sys.path.insert(0, str(root_lib_path))\n","else:\n","    import local_lib_import"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-29T10:33:47.667109Z","iopub.status.busy":"2021-12-29T10:33:47.666688Z","iopub.status.idle":"2021-12-29T10:33:50.598783Z","shell.execute_reply":"2021-12-29T10:33:50.597812Z","shell.execute_reply.started":"2021-12-29T10:33:47.667015Z"},"trusted":true},"outputs":[],"source":["import cv2\n","from face2anime.networks import CycleGenerator, default_decoder, default_encoder, Img2ImgGenerator\n","from face2anime.train_utils import FullyAveragedModel\n","from fastai.vision.all import *\n","import numpy as np\n","import pandas as pd\n","import timeit"]},{"cell_type":"markdown","metadata":{},"source":["# Helper methods"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_img_from_path(p):\n","    return cv2.imread(str(p))\n","\n","def get_file_from_url(url):\n","    !wget $url\n","    path = url[len(url) - ''.join(reversed(url)).index('/'):]\n","    return path\n","\n","def get_img_from_url(url):\n","    path = get_file_from_url(url)\n","    return get_img_from_path(path)\n","\n","def bgr_to_rgb(img):\n","    return img[:,:,::-1]\n","\n","def show_cv_img(img):\n","    plt.imshow(bgr_to_rgb(img))"]},{"cell_type":"markdown","metadata":{},"source":["# Load model"]},{"cell_type":"markdown","metadata":{},"source":["First, we are going to load the Pytorch module that we need to optimize.\n","\n","Set `G_MODEL_PATH` to the path of the weights file of your model. It is assumed that is has been trained and saved with [this notebook](face2anime-bidirectional.ipynb)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T10:33:50.601931Z","iopub.status.busy":"2021-12-29T10:33:50.601327Z","iopub.status.idle":"2021-12-29T10:33:50.609973Z","shell.execute_reply":"2021-12-29T10:33:50.609008Z","shell.execute_reply.started":"2021-12-29T10:33:50.601878Z"},"trusted":true},"outputs":[],"source":["G_MODEL_PATH = Path('/kaggle/input/face2animeproduction/face2anime_bidir_17kds_tr2h2_425ep_ema.pth')\n","G_IMG_SZ = 64\n","G_NCH = 3\n","G_BS = 1\n","G_NORM_MEAN = 0.5\n","G_NORM_STD = 0.5\n","\n","\n","class GenDirection(Enum):\n","    FACE2ANIME = 0\n","    ANIME2FACE = 1\n","\n","\n","def get_dblock():\n","    normalize_tf = Normalize.from_stats(torch.tensor([G_NORM_MEAN]*3), torch.tensor([G_NORM_STD]*3))\n","    return DataBlock(blocks=(ImageBlock, ImageBlock),\n","                     get_items=get_image_files,\n","                     item_tfms=Resize(G_IMG_SZ, method=ResizeMethod.Crop),\n","                     batch_tfms=[normalize_tf],\n","                     splitter=IndexSplitter([]))\n","\n","\n","def get_dataloaders(path):\n","    return get_dblock().dataloaders(path, bs=1)\n","\n","\n","def remove_sn(module):\n","    for m in module.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","            torch.nn.utils.remove_spectral_norm(m)\n","\n","\n","def load_generator_learn(dls, img_size, n_channels, gen_dir=GenDirection.FACE2ANIME, mid_mlp_depth=2, \n","                         norm=NormType.Batch, latent_sz=100, device='cpu'):\n","    def _decoder_builder(imsz, nch, latsz, hooks_by_sz=None): \n","        return default_decoder(imsz, nch, latsz, norm_type=norm, hooks_by_sz=hooks_by_sz)\n","    generators = [Img2ImgGenerator(img_size, n_channels, mid_mlp_depth=mid_mlp_depth, skip_connect=True,\n","                                   encoder=default_encoder(img_size, n_channels, latent_sz, norm_type=norm),\n","                                   decoder_builder=_decoder_builder)\n","                  for _ in range(2)]\n","    generator = FullyAveragedModel(CycleGenerator(*generators))\n","\n","    load_model(G_MODEL_PATH, generator, None, with_opt=False, device=device)\n","    g_module = generator.module.g_a2b if gen_dir == GenDirection.FACE2ANIME else generator.module.g_b2a\n","    learn = Learner(dls, g_module, loss_func=lambda *args: torch.tensor(0.))\n","    return learn"]},{"cell_type":"markdown","metadata":{},"source":["`ds_path` must be set to the parent path of an anime faces dataset.<br>\n","`celeba_path` must be set to the parent path of the CelebA dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T10:33:50.611484Z","iopub.status.busy":"2021-12-29T10:33:50.611188Z","iopub.status.idle":"2021-12-29T10:33:53.092368Z","shell.execute_reply":"2021-12-29T10:33:53.091296Z","shell.execute_reply.started":"2021-12-29T10:33:50.611445Z"},"trusted":true},"outputs":[],"source":["ds_path = Path('/kaggle/input/animecharacterfaces/animeface-character-dataset/data')\n","celeba_path = Path('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/')\n","ds_path.ls()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dls = get_dataloaders(ds_path)\n","learn_f2a = load_generator_learn(dls, G_IMG_SZ, G_NCH, gen_dir=GenDirection.FACE2ANIME)\n","learn_a2f = load_generator_learn(dls, G_IMG_SZ, G_NCH, gen_dir=GenDirection.ANIME2FACE)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference with fastai and Pytorch (used to compare results with optimized models)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FastaiInferenceEngine:\n","    def __init__(self, learn):\n","        self.learn = learn\n","        \n","    @torch.no_grad()    \n","    def __call__(self, cv_img):\n","        np_img = bgr_to_rgb(cv_img)\n","        dl = self.learn.dls.test_dl([np_img], bs=1, num_workers=0)\n","        inp, _, _, dec_preds = self.learn.get_preds(dl=dl, with_input=True, with_decoded=True)\n","        _, dec_preds = self.learn.dls.decode_batch((inp,)+tuplify(dec_preds))[0]\n","        return dec_preds.numpy()\n","        \n","\n","def quick_encode(np_img):\n","    return ((TensorImage(np_img.copy()).permute(2, 0, 1).float()/255 - G_NORM_MEAN)/G_NORM_STD)[None]\n","        \n","        \n","class TorchQuickInferenceEngine:\n","    def __init__(self, model:nn.Module):\n","        self.model = model\n","        \n","    def _decode_torch_out(self, preds_t):\n","        return ((preds_t[0] * G_NORM_STD + G_NORM_MEAN) * 255).int()\n","        \n","    @torch.no_grad()\n","    def __call__(self, cv_img):\n","        downsample = cv_img.shape[0] > G_IMG_SZ\n","        # It's not clear what method is better when one dimension is bigger and the other is smaller,\n","        # so I'm only looking at height\n","        inter_method = cv2.INTER_AREA if downsample else cv2.INTER_LINEAR\n","        cv_img = cv2.resize(cv_img,\n","                            dsize=(G_IMG_SZ, G_IMG_SZ),\n","                            interpolation=inter_method)\n","        np_img = bgr_to_rgb(cv_img)\n","        inp = quick_encode(np_img)\n","        self.model.eval()\n","        preds_t = self.model(inp)\n","        dec_preds = self._decode_torch_out(preds_t) \n","        return dec_preds.numpy()\n","    \n","    @classmethod\n","    def from_fastai_learn(cls, learn):\n","        return cls(learn.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_img = get_img_from_path(celeba_path/'000002.jpg')\n","show_cv_img(sample_img)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fastai_out = FastaiInferenceEngine(learn_f2a)(sample_img)\n","TensorImage(fastai_out).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learn_f2a.model.eval()\n","torch_out = TorchQuickInferenceEngine.from_fastai_learn(learn_f2a)(sample_img)\n","TensorImage(torch_out).show()"]},{"cell_type":"markdown","metadata":{},"source":["Some differences between the PyTorch and the fastai output could appear due to different preprocessing."]},{"cell_type":"markdown","metadata":{},"source":["# ONNX Runtime optimized model\n","\n","ONNX is an open format built to represent machine learning models. There are converters from almost any machine learning framework to this format.\n","\n","ONNX makes it easier to leverage hardware optimizations without needing them to be compatible with your prefered framework.\n","\n","ONNX Runtime is an accelerator for machine learning models in ONNX format."]},{"cell_type":"markdown","metadata":{},"source":["## Export to ONNX format"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def export_to_onnx(model:nn.Module, out_path):\n","    model.eval()\n","    \n","    # Needed for ONNX compatibility and doesn't affect output\n","    remove_sn(model)\n","    \n","    torch.onnx.export(model, \n","                      (torch.rand(1, G_NCH, G_IMG_SZ, G_IMG_SZ),), \n","                      out_path,\n","                      opset_version=11)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["export_to_onnx(learn_f2a.model, 'f2a_model.onnx')\n","export_to_onnx(learn_a2f.model, 'a2f_model.onnx')"]},{"cell_type":"markdown","metadata":{},"source":["## Run ONNX model using ONNX Runtime\n","\n","You need to install ONNX and ONNX Runtime. Note that ONNX Runtime is compatible with Python versions 3.5 to 3.7."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install onnx\n","!pip install onnxruntime"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import onnx, onnxruntime\n","onnx_model = onnx.load(\"f2a_model.onnx\")\n","onnx.checker.check_model(onnx_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ONNXInferenceEngine():\n","    def __init__(self, ort_session):\n","        self.ort_session = ort_session\n","        \n","    def _decode_onnx_out(self, onnx_preds):\n","        return ((onnx_preds[0] * G_NORM_STD + G_NORM_MEAN) * 255).astype(int)#int()\n","        \n","    def __call__(self, hwc_bgr_img):\n","        downsample = hwc_bgr_img.shape[0] > G_IMG_SZ\n","        # It's not clear what method is better when one dimension is bigger and the other is smaller,\n","        # so I'm only looking at height\n","        inter_method = cv2.INTER_AREA if downsample else cv2.INTER_LINEAR\n","        hwc_bgr_img = cv2.resize(hwc_bgr_img,\n","                                 dsize=(G_IMG_SZ, G_IMG_SZ),\n","                                 interpolation=inter_method)\n","        hwc_rgb_img = bgr_to_rgb(hwc_bgr_img)\n","        inp = quick_encode(hwc_rgb_img).numpy()\n","    \n","        ort_inputs = {self.ort_session.get_inputs()[0].name: inp}\n","        ort_outs = self.ort_session.run(None, ort_inputs) # -> 1 item list\n","\n","        dec_ort_outs = self._decode_onnx_out(ort_outs[0]) \n","        return dec_ort_outs\n","    \n","    @classmethod\n","    def from_model_path(cls, model_path):\n","        ort_session = onnxruntime.InferenceSession(model_path)\n","        return cls(ort_session)"]},{"cell_type":"markdown","metadata":{},"source":["Test ONNX Runtime with a sample image:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["onnx_rt_out = ONNXInferenceEngine.from_model_path(\"f2a_model.onnx\")(sample_img)\n","TensorImage(onnx_rt_out).show()"]},{"cell_type":"markdown","metadata":{},"source":["Compare ONNX model output with Pytorch model output:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.abs(onnx_rt_out - torch_out).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.where(np.abs(onnx_rt_out - torch_out) > 0)"]},{"cell_type":"markdown","metadata":{},"source":["# OpenVINO optimized models"]},{"cell_type":"markdown","metadata":{},"source":["[OpenVINO](https://docs.openvino.ai/latest/index.html) is an open-source toolkit for optimizing and deploying AI inference. \n","\n","Its inference engine **only supports Intel devices**. Moreover, it only accepts models with ONNX and OpenVINO IR formats.\n","\n","The OpenVINO IR is an intermediate representation of a model adjusted for optimal performance when inferred with the Inference Engine. \n","\n","You can convert a model to IR using the [OpenVINO Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html). For PyTorch models, you first need to convert the model to ONNX and then to OpenVINO IR. You could skip the last step but it's not recommended.\n","\n","The IR of a model consists of a pair of files describing the model:\n","- .xml - Describes the network topology\n","- .bin - Contains the weights and biases binary data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def install_open_vino():\n","    !add-apt-repository -y ppa:deadsnakes/ppa\n","    !apt-get install -y python3.7-dev\n","    !python3 -m venv openvino_env\n","    !source openvino_env/bin/activate\n","    !python -m pip install --upgrade pip\n","    !pip install openvino-dev[onnx]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["install_open_vino()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from openvino.inference_engine import IECore\n","\n","\n","class OpenVINOInferenceEngine():\n","    def __init__(self, model_path, weights_path=''):\n","        self.ie = IECore()\n","        net = self.ie.read_network(model=model_path, weights=weights_path)\n","        self.exec_net = self.ie.load_network(network=net, device_name=\"CPU\")\n","        self.input_layer = next(iter(self.exec_net.input_info))\n","        self.output_layer = next(iter(self.exec_net.outputs))\n","        \n","    def _decode_out(self, onnx_preds):\n","        return ((onnx_preds[0] * G_NORM_STD + G_NORM_MEAN) * 255).astype(int)#int()\n","        \n","    def __call__(self, hwc_bgr_img):\n","        downsample = hwc_bgr_img.shape[0] > G_IMG_SZ\n","        # TODO: don't know what method is better when one dimension is bigger and the other is smaller,\n","        # so I'm only looking at height\n","        inter_method = cv2.INTER_AREA if downsample else cv2.INTER_LINEAR\n","        hwc_bgr_img = cv2.resize(hwc_bgr_img,\n","                                 dsize=(G_IMG_SZ, G_IMG_SZ),\n","                                 interpolation=inter_method)\n","        hwc_rgb_img = bgr_to_rgb(hwc_bgr_img)\n","        inp = quick_encode(hwc_rgb_img).numpy()\n","    \n","        preds = self.exec_net.infer(inputs={self.input_layer: inp})\n","        preds = preds[self.output_layer]\n","\n","        dec_preds = self._decode_out(preds) \n","        return dec_preds"]},{"cell_type":"markdown","metadata":{},"source":["## Inference with OpenVINO IE and ONNX model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ov_onnx_out = OpenVINOInferenceEngine('f2a_model.onnx')(sample_img)\n","TensorImage(ov_onnx_out).show()"]},{"cell_type":"markdown","metadata":{},"source":["Compare with the output of the Pytorch model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.abs(ov_onnx_out - torch_out).sum()"]},{"cell_type":"markdown","metadata":{},"source":["## Inference with OpenVINO IE and OpenVINO IR model"]},{"cell_type":"markdown","metadata":{},"source":["### Convert ONNX model to OpenVINO IR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["openvino_ir_model_path = Path('./openvino_model')\n","!mo --input_model f2a_model.onnx --output_dir $openvino_ir_model_path --batch 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls $openvino_ir_model_path"]},{"cell_type":"markdown","metadata":{},"source":["### Perform inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ov_out = OpenVINOInferenceEngine(openvino_ir_model_path/'f2a_model.xml')(sample_img)\n","TensorImage(ov_out).show()"]},{"cell_type":"markdown","metadata":{},"source":["Compare with PyTorch generated image:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.abs(ov_out - torch_out).sum()"]},{"cell_type":"markdown","metadata":{},"source":["# Test optimized model with a pipeline of face detection + conversion"]},{"cell_type":"markdown","metadata":{},"source":["## Face detection"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T10:33:53.094408Z","iopub.status.busy":"2021-12-29T10:33:53.094173Z","iopub.status.idle":"2021-12-29T10:33:53.098314Z","shell.execute_reply":"2021-12-29T10:33:53.097343Z","shell.execute_reply.started":"2021-12-29T10:33:53.094379Z"},"trusted":true},"outputs":[],"source":["DETECTOR_TARGET_SZ = (300, 300)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T10:33:53.100039Z","iopub.status.busy":"2021-12-29T10:33:53.099752Z","iopub.status.idle":"2021-12-29T10:33:53.195908Z","shell.execute_reply":"2021-12-29T10:33:53.194792Z","shell.execute_reply.started":"2021-12-29T10:33:53.099999Z"},"trusted":true},"outputs":[],"source":["class CaffeFaceDetectorDnn:\n","    def __init__(self):\n","        # model structure\n","        !wget https://github.com/opencv/opencv/raw/3.4.0/samples/dnn/face_detector/deploy.prototxt\n","        #pre-trained weights: \n","        !wget https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n","        self.detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n","        \n","    def __call__(self, inp):\n","        self.detector.setInput(inp)\n","        return self.detector.forward()\n","    \n","    \n","class OpenVINOFaceDetectorDnn():\n","    def __init__(self, model_path, weights_path=''):\n","        self.ie = IECore()\n","        net = self.ie.read_network(model=model_path, weights=weights_path)\n","        self.exec_net = self.ie.load_network(network=net, device_name=\"CPU\")\n","        self.input_layer = next(iter(self.exec_net.input_info))\n","        self.output_layer = next(iter(self.exec_net.outputs))\n","        \n","    def __call__(self, inp):\n","        return self.exec_net.infer(inputs={self.input_layer: inp})[self.output_layer]\n","\n","\n","class OpenCVFaceDetector:\n","    def __init__(self, detector=None):\n","        self.detector = detector\n","    \n","    def _prepare_for_detector(self, img):\n","        resized_img = cv2.resize(img, DETECTOR_TARGET_SZ)\n","        tfm_aspect_ratio = [img.shape[i]/resized_img.shape[i] for i in (0, 1)]\n","        img_blob = cv2.dnn.blobFromImage(image=resized_img)\n","        return img_blob, tfm_aspect_ratio\n","    \n","    def _get_detector(self):\n","        if self.detector is None:\n","            self.detector = CaffeFaceDetectorDnn()\n","        return self.detector\n","    \n","    def detect(self, img:np.array, thresh=0.9):\n","        orig_img = img\n","        img, tfm_aspect_ratio = self._prepare_for_detector(img)\n","        detector = self._get_detector()\n","        detector_out = detector(img)\n","        detections = OpenCVDetectionList(detector_out, tfm_aspect_ratio, thresh=thresh)\n","        return detections\n","\n","\n","class OpenCVDetectionList():\n","    def __init__(self, detector_out, tfm_aspect_ratio, thresh=0.9):\n","        column_labels = [\"img_id\", \"is_face\", \"confidence\", \"left\", \"top\", \"right\", \"bottom\"]\n","        df = pd.DataFrame(detector_out[0][0], columns=column_labels)\n","        df = df[df.confidence > thresh]\n","        self.df = self._scale_coords_to_orig_sz(df, tfm_aspect_ratio)\n","        \n","    def _scale_coords_to_orig_sz(self, df, aspect_ratio):\n","        orig_width = int(DETECTOR_TARGET_SZ[1] * aspect_ratio[1])\n","        orig_height = int(DETECTOR_TARGET_SZ[0] * aspect_ratio[0])\n","        for _, det in df.iterrows():\n","            det.left = min((det.left * orig_width).astype(int), orig_width)\n","            det.top = min((det.top * orig_height).astype(int), orig_height)\n","            det.right = min((det.right * orig_width).astype(int), orig_width)\n","            det.bottom = min((det.bottom * orig_height).astype(int), orig_height)\n","        return df\n","        \n","\n","def show_detected_images(source_img, det_list):\n","    for _, det in det_list.df.iterrows():\n","        l, t, r, b = [int(coord) for coord in (det.left, det.top, det.right, det.bottom)]\n","        is_empty = (l >= r) or (t >= b)\n","        if not is_empty: \n","            show_cv_img(source_img[t:b, l:r])\n","        plt.figure()\n","        \n","        \n","def show_detected_boxes(source_img, det_list):\n","    img = source_img.copy()\n","    for _, det in det_list.df.iterrows():\n","        l, t, r, b = [int(coord) for coord in (det.left, det.top, det.right, det.bottom)]\n","        is_empty = (l >= r) or (t >= b)\n","        if not is_empty: \n","            cv2.rectangle(img, (l, t), (r, b), (50, 50, 50), 1)\n","    show_cv_img(img)"]},{"cell_type":"markdown","metadata":{},"source":["Test the face detector:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T12:08:10.653193Z","iopub.status.busy":"2021-12-29T12:08:10.652895Z","iopub.status.idle":"2021-12-29T12:08:10.819548Z","shell.execute_reply":"2021-12-29T12:08:10.818587Z","shell.execute_reply.started":"2021-12-29T12:08:10.653162Z"},"trusted":true},"outputs":[],"source":["detector = OpenCVFaceDetector()\n","# detector2 = OpenCVFaceDetector(OpenVINOFaceDetectorDnn(\n","#     openvino_ir_model_path/'res10_300x300_ssd_iter_140000.xml',\n","#     openvino_ir_model_path/'res10_300x300_ssd_iter_140000.bin'\n","# ))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T10:50:12.065551Z","iopub.status.busy":"2021-12-29T10:50:12.064383Z","iopub.status.idle":"2021-12-29T10:50:12.850662Z","shell.execute_reply":"2021-12-29T10:50:12.849744Z","shell.execute_reply.started":"2021-12-29T10:50:12.06547Z"},"trusted":true},"outputs":[],"source":["img = get_img_from_path(ds_path.ls()[6])\n","show_cv_img(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T12:08:15.09791Z","iopub.status.busy":"2021-12-29T12:08:15.097532Z","iopub.status.idle":"2021-12-29T12:08:21.96925Z","shell.execute_reply":"2021-12-29T12:08:21.968063Z","shell.execute_reply.started":"2021-12-29T12:08:15.097868Z"},"trusted":true},"outputs":[],"source":["det_list = detector.detect(img, 0.9)\n","# det_list2 = detector2.detect(img, 0.9)\n","det_list.df #, det_list2.df"]},{"cell_type":"markdown","metadata":{},"source":["If you have any issues with pandas, run: `!pip install pandas==1.3.5`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-28T21:43:40.055654Z","iopub.status.busy":"2021-12-28T21:43:40.055328Z","iopub.status.idle":"2021-12-28T21:43:40.286158Z","shell.execute_reply":"2021-12-28T21:43:40.285551Z","shell.execute_reply.started":"2021-12-28T21:43:40.055615Z"},"trusted":true},"outputs":[],"source":["show_detected_images(img, det_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-28T21:43:41.46367Z","iopub.status.busy":"2021-12-28T21:43:41.463213Z","iopub.status.idle":"2021-12-28T21:43:41.63335Z","shell.execute_reply":"2021-12-28T21:43:41.632466Z","shell.execute_reply.started":"2021-12-28T21:43:41.463629Z"},"trusted":true},"outputs":[],"source":["show_detected_boxes(img, det_list)"]},{"cell_type":"markdown","metadata":{},"source":["## (Optional) Convert Caffe face detection model to OpenVINO IR"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T10:51:31.483684Z","iopub.status.busy":"2021-12-29T10:51:31.482905Z","iopub.status.idle":"2021-12-29T10:51:44.386637Z","shell.execute_reply":"2021-12-29T10:51:44.385886Z","shell.execute_reply.started":"2021-12-29T10:51:31.483632Z"},"trusted":true},"outputs":[],"source":["!mo --input_model res10_300x300_ssd_iter_140000.caffemodel --input_proto deploy.prototxt --output_dir $openvino_ir_model_path"]},{"cell_type":"markdown","metadata":{},"source":["## Ensemble detect + replace"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T12:03:36.628399Z","iopub.status.busy":"2021-12-29T12:03:36.628001Z","iopub.status.idle":"2021-12-29T12:03:36.660597Z","shell.execute_reply":"2021-12-29T12:03:36.659851Z","shell.execute_reply.started":"2021-12-29T12:03:36.628363Z"},"trusted":true},"outputs":[],"source":["def expand_box(ltrb_coords, orig_shape, extra_w_pct=0.25, extra_h_pct=0.25,\n","               enforce_symmetry=False):\n","    # bottom and right coordinates must be interpreted as the first indexes out\n","    # of the image, so that numpy indexing logic is matched and one would index \n","    # img[t:b, l:r]\n","    l, t, r, b = ltrb_coords\n","    orig_h, orig_w, *_ = orig_shape\n","    w = r - l\n","    h = b - t\n","    \n","    extra_w = w * extra_w_pct\n","    horizontal_inc = (min(round(extra_w/2), l, orig_w - r) if enforce_symmetry else round(extra_w/2))\n","    l = max(0, l - horizontal_inc)\n","    r = min(orig_w, r + horizontal_inc)\n","    \n","    extra_h = h * extra_h_pct\n","    vertical_inc = (min(round(extra_h/2), t, orig_h - b) if enforce_symmetry else round(extra_h/2))\n","    t = max(0, t - vertical_inc)\n","    b = min(orig_h, b + vertical_inc)\n","       \n","    return l, t, r, b\n","  \n","\n","assert expand_box((5, 6, 65, 66), (100, 100)) == (0, 0, 73, 74)\n","assert expand_box((5, 6, 65, 66), (100, 100), enforce_symmetry=True) == (0, 0, 70, 72)\n","assert expand_box((35, 36, 95, 96), (100, 100)) == (27, 28, 100, 100)\n","assert expand_box((35, 36, 95, 96), (100, 100), enforce_symmetry=True) == (30, 32, 100, 100)\n","\n","\n","def pad_to_make_square(img:np.array, pad_mode='edge', **pad_kwargs):\n","    h, w, *_ = img.shape\n","    if h == w: return img, (0, 0, w, h)\n","    \n","    target_sz = max(h, w)\n","    if target_sz == h:\n","        vertical_pad = (0, 0)\n","        horizontal_inc = target_sz - w\n","        horizontal_pad = (horizontal_inc//2, horizontal_inc//2 + horizontal_inc%2)\n","    else:\n","        horizontal_pad = (0, 0)\n","        vertical_inc = target_sz - h\n","        vertical_pad = (vertical_inc//2, vertical_inc//2 + vertical_inc%2)\n","    extra_axis = len(img.shape) - 2\n","    padded_img = np.pad(img, (vertical_pad, horizontal_pad, *[(0, 0)]*extra_axis), \n","                        mode=pad_mode, **pad_kwargs)\n","    orig_img_ltrb_coords = (horizontal_pad[0], \n","                            vertical_pad[0], \n","                            horizontal_pad[0] + w,\n","                            vertical_pad[0] + h)\n","    return padded_img, orig_img_ltrb_coords\n","\n","\n","def test_pad_to_make_square():\n","    in_row_v = np.array([[1, 2, 3]])\n","    expected_padded_row_v = np.array([[0]*3, [1, 2, 3], [0]*3])\n","    expected_row_v_coords = (0, 1, 3, 2)\n","    actual_padded_row_v, actual_row_v_coords = pad_to_make_square(in_row_v, pad_mode='constant', constant_values=0)\n","    assert (actual_padded_row_v == expected_padded_row_v).all()\n","    assert actual_row_v_coords == expected_row_v_coords\n","    \n","    in_col_v = np.array([[1], [2], [3]])\n","    expected_padded_col_v = np.array([[0, 1, 0], [0, 2, 0], [0, 3, 0]])\n","    expected_col_v_coords = (1, 0, 2, 3)\n","    actual_padded_col_v, actual_col_v_coords = pad_to_make_square(in_col_v, pad_mode='constant', constant_values=0)\n","    assert (actual_padded_col_v == expected_padded_col_v).all()\n","    assert actual_col_v_coords == expected_col_v_coords\n","    \n","\n","test_pad_to_make_square()\n","        \n","\n","def transform_img(inference_engine, img, detected_faces):\n","    out_img = img.copy()\n","    # Iter in reversed order to ensure the most confident appears on top in case of overlapping\n","    for _, det in detected_faces.df[::-1].iterrows():\n","        l, t, r, b = [int(coord) for coord in (det.left, det.top, det.right, det.bottom)]        \n","        is_empty = (l >= r) or (t >= b)\n","        if not is_empty: \n","            l, t, r, b = expand_box((l, t, r, b), img.shape, extra_w_pct=0.4, extra_h_pct=0.4)\n","            face_subimg = img[t:b, l:r]\n","            \n","            # Make square to prevent fastai from cropping interesting parts of the image\n","            face_subimg, coords_before_pad = pad_to_make_square(face_subimg)\n","            \n","            pred = inference_engine(face_subimg)\n","            \n","            #pred = pred[[2, 1, 0]].permute(1, 2, 0).numpy().astype(np.uint8)\n","            pred_hwc_bgr = pred[[2, 1, 0]].transpose(1, 2, 0).astype(np.uint8)\n","            pred_hwc_bgr = cv2.resize(pred_hwc_bgr,\n","                                      dsize=(face_subimg.shape[1], face_subimg.shape[0]),\n","                                      interpolation=cv2.INTER_LINEAR)\n","            pred_hwc_bgr = pred_hwc_bgr[coords_before_pad[1]:coords_before_pad[3], coords_before_pad[0]:coords_before_pad[2]]\n","            #plt.figure()\n","            #show_cv_img(pred_np)\n","            out_img[t:b, l:r, :] = pred_hwc_bgr\n","            \n","    return out_img\n","\n","\n","def convert_img(inference_engine, img, detector=None, det_thresh=0.9):      \n","    if detector is None: detector = OpenCVFaceDetector()\n","    det_list = detector.detect(img, 0.9) \n","    return transform_img(inference_engine, img, det_list)\n","\n","\n","def convert_video(learn, vid_path, out_path, detector=None, det_thresh=0.9):\n","    if detector is None: detector = OpenCVFaceDetector()\n","        \n","    vid_capture = cv2.VideoCapture(vid_path)\n","    output = None\n","\n","    if (vid_capture.isOpened() == False):\n","        print(\"Error opening the video file\")\n","    else:\n","        fps = vid_capture.get(cv2.CAP_PROP_FPS)\n","        print('Frames per second : ', fps,'FPS')\n","\n","        frame_count = vid_capture.get(cv2.CAP_PROP_FRAME_COUNT)\n","        print('Frame count : ', frame_count)\n","        \n","        frame_width = int(vid_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frame_height = int(vid_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        frame_size = (frame_width,frame_height)\n","\n","        output = cv2.VideoWriter(out_path, \n","                                 cv2.VideoWriter_fourcc('M','J','P','G'), \n","                                 20, \n","                                 frame_size)\n","\n","    frame_idx = 0\n","    while(vid_capture.isOpened()):\n","        ok, frame = vid_capture.read()\n","        if ok:\n","            #cv2.imshow('Frame',frame)\n","            #show_cv_img(frame)\n","            #plt.figure()\n","            \n","            converted_frame = convert_img(learn, frame, detector=detector, det_thresh=det_thresh)\n","            \n","            output.write(converted_frame)\n","            if (frame_idx + 1) % 50 == 0: print('Written frame ', frame_idx)\n","        else:\n","            print('Error in frame ', frame_idx)\n","            break\n","        frame_idx += 1\n","\n","    vid_capture.release()\n","    if output is not None: output.release()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["detector = OpenCVFaceDetector()"]},{"cell_type":"markdown","metadata":{},"source":["Test pipeline with an example image:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_out_img = convert_img(#ONNXInferenceEngine.from_model_path('f2a_model.onnx'), \n","                           #TorchQuickInferenceEngine.from_fastai_learn(learn_f2a),\n","                           FastaiInferenceEngine(learn_f2a),\n","                           get_img_from_path(celeba_path.ls()[0]),\n","                           #get_img_from_url('https://[REPLACE WITH URL OF EXAMPLE IMAGE]'), \n","                           detector=detector)\n","show_cv_img(test_out_img)"]},{"cell_type":"markdown","metadata":{},"source":["Test pipeline with an example video:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["convert_video(learn_f2a, '[REPLACE WITH PATH OF A SAMPLE VIDEO]', './test_out1.avi', \n","              detector=detector,\n","              det_thresh=0.5)"]},{"cell_type":"markdown","metadata":{},"source":["## Performance comparison"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_img = get_img_from_path(celeba_path.ls()[0])\n","show_cv_img(test_img)\n","test_img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fastai_inf_eng = FastaiInferenceEngine(learn_f2a)\n","%timeit -r 5 -n 3 with learn_f2a.no_bar(): convert_img(fastai_inf_eng, test_img, detector=detector)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-28T22:59:40.360131Z","iopub.status.busy":"2021-12-28T22:59:40.359826Z","iopub.status.idle":"2021-12-28T22:59:47.464993Z","shell.execute_reply":"2021-12-28T22:59:47.463946Z","shell.execute_reply.started":"2021-12-28T22:59:40.360096Z"},"trusted":true},"outputs":[],"source":["torch_inf_eng = TorchQuickInferenceEngine.from_fastai_learn(learn_f2a)\n","%timeit -r 5 -n 3 convert_img(torch_inf_eng, test_img, detector=detector)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["onnx_inf_eng = ONNXInferenceEngine.from_model_path('f2a_model.onnx')\n","%timeit -r 5 -n 3 convert_img(onnx_inf_eng, test_img, detector=detector)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ov_onnx_inf_eng = OpenVINOInferenceEngine('f2a_model.onnx')\n","%timeit -r 5 -n 3 convert_img(ov_onnx_inf_eng, test_img, detector=detector)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-29T12:08:48.090527Z","iopub.status.busy":"2021-12-29T12:08:48.089237Z","iopub.status.idle":"2021-12-29T12:08:52.68632Z","shell.execute_reply":"2021-12-29T12:08:52.685391Z","shell.execute_reply.started":"2021-12-29T12:08:48.090455Z"},"trusted":true},"outputs":[],"source":["ov_inf_eng = OpenVINOInferenceEngine(openvino_ir_model_path/'f2a_model.xml')\n","%timeit -r 5 -n 3 convert_img(ov_inf_eng, test_img, detector=detector)"]},{"cell_type":"markdown","metadata":{},"source":["To get a detailed report of the execution times by method, use `%prun`. For instance:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%prun -s cumulative convert_img(fastai_inf_eng, test_img, detector=detector)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}
