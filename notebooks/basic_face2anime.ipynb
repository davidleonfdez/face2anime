{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from functools import partial\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vgg import vgg19\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "img_size = 224\n",
    "n_channels = 3\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "animecharacterfaces, by Kaggle user *aadilmalik94*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#anime_faces_path = Path('/kaggle/input/anime-faces-safebooru/anime-faces').resolve()\n",
    "anime_faces_path = Path('/kaggle/input/animecharacterfaces/animeface-character-dataset/data').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_path = Path('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba')\n",
    "#input_fns = get_image_files(celeba_path)\n",
    "# get_image_files is too slow, there's no need to check the extension here\n",
    "input_fns = celeba_path.ls()\n",
    "input_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ds path passed to `dblock.dataloaders()` or `ImageDataLoaders.from_dblock()` will be forwarded\n",
    "to `get_items`, which will return a list of items, usually a list of image paths if `get_items=get_image_files`.\n",
    "\n",
    "So, for each item, we are expected to receive a filename `fn` and be able to\n",
    "derive x and y from it, with `get_x(fn)` and `get_y(fn)`.\n",
    "\n",
    "For unpaired image to image translation, we can:\n",
    "* Use the target images ds path as the DataBlock `source`. Then, `get_y` can just return the path received.\n",
    "* Load independently the filenames of the input images ds; let's call it `input_fns`. Then, `get_x` would need to return a random item from `input_fns`. `get_x` is called every time a data item is used; so, by using random, we can be sure every x is not tied to a fixed y; i.e., they won't be together in the same (x, y) batch every epoch for loss calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_input(fn):\n",
    "    return input_fns[random.randint(0, len(input_fns)-1)]\n",
    "    \n",
    "dblock = DataBlock(blocks = (ImageBlock, ImageBlock),\n",
    "                   get_x = get_random_input,\n",
    "                   get_items = get_image_files,\n",
    "                   splitter = IndexSplitter([]),\n",
    "                   item_tfms=Resize(img_size, method=ResizeMethod.Crop), \n",
    "                   batch_tfms = Normalize.from_stats(torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])))\n",
    "main_path = anime_faces_path\n",
    "dls = dblock.dataloaders(main_path, path=main_path, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesCalculator:\n",
    "    def __init__(self, vgg_style_layers_idx:List[int], vgg_content_layers_idx:List[int],\n",
    "                 vgg:nn.Module=None, normalize_inputs=False, device:torch.device=None):\n",
    "        self.vgg = vgg19(pretrained=True) if vgg is None else vgg\n",
    "        self.vgg.eval()\n",
    "        if device is not None: self.vgg.to(device)\n",
    "        modules_to_hook = [self.vgg.features[idx] for idx in (*vgg_style_layers_idx, *vgg_content_layers_idx)]\n",
    "        self.hooks = hook_outputs(modules_to_hook, detach=False)\n",
    "        self.style_ftrs_hooks = self.hooks[:len(vgg_style_layers_idx)]\n",
    "        self.content_ftrs_hooks = self.hooks[len(vgg_style_layers_idx):]\n",
    "        self.normalize_inputs = normalize_inputs\n",
    "        # TODO: when to remove hooks??? `clean` method????\n",
    "    \n",
    "    def _get_hooks_out(self, hooks):\n",
    "        return [h.stored for h in hooks]\n",
    "    \n",
    "    def _forward(img_t:torch.Tensor):\n",
    "        if self.normalize_inputs: \n",
    "            mean, std = fastai.vision.imagenet_stats\n",
    "            img_t = fastai.vision.normalize(img_t, torch.tensor(mean), torch.tensor(std))\n",
    "        self.vgg(img_t)\n",
    "    \n",
    "    def calc_style(self, img_t:torch.Tensor) -> List[torch.Tensor]:\n",
    "        self.vgg(img_t)\n",
    "        return self._get_hooks_out(self.style_ftrs_hooks)\n",
    "    \n",
    "    def calc_content(self, img_t:torch.Tensor) -> List[torch.Tensor]:\n",
    "        self.vgg(img_t)\n",
    "        return self._get_hooks_out(self.content_ftrs_hooks)\n",
    "    \n",
    "    def calc_style_and_content(self, img_t:torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        self.vgg(img_t)\n",
    "        style_ftrs = self._get_hooks_out(self.style_ftrs_hooks)\n",
    "        content_ftrs = self._get_hooks_out(self.content_ftrs_hooks)\n",
    "        return style_ftrs, content_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_content_layers_idx = [22]\n",
    "ftrs_calc = FeaturesCalculator([], vgg_content_layers_idx, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, there are two versions of content loss, with the same behaviour but different implementation:\n",
    "* A functional version, returned by `get_content_loss`. A callback parameter is needed \n",
    "* A callback version. Although less intuitive, this may be preferable because it stores the loss value inside `learner.loss_func.content_loss`, making it accessible to the metrics display system. If you want the learner to display the content loss every epoch, it only requires passing `metrics=['content_loss', ...]` to GANLearner.init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n",
    "def get_content_loss(last_input_cb):\n",
    "    def _content_loss(output, target):\n",
    "        input_content_ftrs = ftrs_calc.calc_content(last_input_cb.x)[0]\n",
    "        output_content_ftrs = ftrs_calc.calc_content(output)[0]\n",
    "        return content_loss_func(output_content_ftrs, input_content_ftrs)\n",
    "    \n",
    "    return _content_loss\n",
    "\n",
    "\n",
    "class ContentLossCallback(Callback):\n",
    "    def __init__(self, weight=1., ftrs_calc=None, device=None):\n",
    "        self.weight = weight\n",
    "        if ftrs_calc is None:\n",
    "            vgg_content_layers_idx = [22]\n",
    "            ftrs_calc = FeaturesCalculator([], vgg_content_layers_idx, device=device)\n",
    "        self.ftrs_calc = ftrs_calc\n",
    "        \n",
    "    def after_loss(self):\n",
    "        if self.gan_trainer.gen_mode:\n",
    "            input_content_ftrs = self.ftrs_calc.calc_content(self.x)[0]\n",
    "            output_content_ftrs = self.ftrs_calc.calc_content(self.pred)[0]\n",
    "            loss_val = content_loss_func(output_content_ftrs, input_content_ftrs)\n",
    "            # Store result inside learn.loss_func to make it visible to metrics display\n",
    "            self.learn.loss_func.content_loss = loss_val            \n",
    "            # This will probably stop working once a new fastai version is released, as backward \n",
    "            # won't be called on learn.loss anymore, but on learn.loss_grad\n",
    "            self.learn.loss += loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are only useful if you decide to use the functional version of content loss and want to combine it with Wasserstein loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_wgan_loss(fake_pred, output, target): return fake_pred.mean()\n",
    "def crit_wgan_loss(real_pred, fake_pred): return real_pred.mean() - fake_pred.mean()\n",
    "\n",
    "\n",
    "def get_gen_wgan_content_loss(last_input_cb, content_loss_w=1.):\n",
    "    content_loss = get_content_loss(last_input_cb)\n",
    "    \n",
    "    def _gen_wgan_content_loss(fake_pred, output, target):\n",
    "        wgan_loss = gen_wgan_loss(fake_pred, output, target) \n",
    "        cont_loss = content_loss(output, target)\n",
    "        return wgan_loss + content_loss_w * cont_loss\n",
    "    \n",
    "    return _gen_wgan_content_loss\n",
    "\n",
    "\n",
    "def create_wgan_w_content_loss_learner(dls, generator, critic, cbs=None, **kwargs):\n",
    "    if cbs is None: cbs = []\n",
    "    last_input_cb=Callback()\n",
    "    cbs.append(last_input_cb)\n",
    "    gen_loss = get_gen_wgan_content_loss(last_input_cb)\n",
    "    return GANLearner(dls, generator, critic, gen_loss, crit_wgan_loss,\n",
    "                      cbs=cbs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANGPCallback(Callback):\n",
    "    def __init__(self, plambda=10., epsilon_sampler=None): \n",
    "        self.plambda = plambda\n",
    "        if epsilon_sampler is None: epsilon_sampler = random_epsilon_gp_sampler\n",
    "        self.epsilon_sampler = epsilon_sampler\n",
    "        \n",
    "    def _gradient_penalty(self, real, fake, plambda, epsilon_sampler):\n",
    "        epsilon = epsilon_sampler(real, fake)\n",
    "        x_hat = epsilon * real + (1 - epsilon) * fake\n",
    "        x_hat_pred = self.model.critic(x_hat).mean()\n",
    "\n",
    "        grads = torch.autograd.grad(outputs=x_hat_pred, inputs=x_hat, create_graph=True)[0]\n",
    "        return plambda * ((grads.norm() - 1)**2)    \n",
    "        \n",
    "    def after_loss(self):\n",
    "        if not self.gan_trainer.gen_mode:\n",
    "            # In critic mode, GANTrainer swaps x and y; so, here x is original y (real target)\n",
    "            real = self.x\n",
    "            assert not self.y.requires_grad\n",
    "            fake = self.model.generator(self.y).requires_grad_(True)\n",
    "            # This will probably stop working once a new fastai version is released, as backward \n",
    "            # won't be called on learn.loss anymore, but on learn.loss_grad\n",
    "            self.learn.loss += self._gradient_penalty(real, fake, self.plambda, self.epsilon_sampler)\n",
    "\n",
    "\n",
    "def random_epsilon_gp_sampler(real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
    "    # A different random value of epsilon for any element of a batch\n",
    "    epsilon_vec = torch.rand(real.shape[0], 1, 1, 1, dtype=torch.float, device=real.device, requires_grad=False)\n",
    "    return epsilon_vec.expand_as(real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_save_model(learner, filename, base_path='/kaggle/working'):\n",
    "    if isinstance(base_path, str): base_path = Path(base_path)\n",
    "    if not isinstance(base_path, Path): raise Exception('Invalid base_path')\n",
    "    file = join_path_file(filename, base_path/learner.model_dir, ext='.pth')\n",
    "    save_model(file, learner.model, learner.opt)\n",
    "    \n",
    "def custom_load_model(learner, filename, with_opt=True, device=None, \n",
    "                      base_path='/kaggle/input/face2anime-weights', **kwargs):\n",
    "    if isinstance(base_path, str): base_path = Path(base_path)\n",
    "    if not isinstance(base_path, Path): raise Exception('Invalid base_path')\n",
    "    if device is None and hasattr(learner.dls, 'device'): device = learner.dls.device\n",
    "    if learner.opt is None: learner.create_opt()\n",
    "    #file = join_path_file(filename, base_path/learner.model_dir, ext='.pth')\n",
    "    file = base_path/f'{filename}.pth'\n",
    "    load_model(file, learner.model, learner.opt, with_opt=with_opt, device=device, **kwargs)\n",
    "    \n",
    "def predict_n(learner, n_imgs, max_bs=64):\n",
    "    dummy_path = Path('')\n",
    "    dl = learner.dls.test_dl([dummy_path]*n_imgs, bs=max_bs)   \n",
    "    inp, imgs_t, _, dec_imgs_t = learner.get_preds(dl=dl, with_input=True, with_decoded=True)\n",
    "    dec_batch = dls.decode_batch((inp,) + tuplify(dec_imgs_t), max_n=n_imgs)\n",
    "    return dec_batch\n",
    "    \n",
    "def predict_show_n(learner, n_imgs, **predict_n_kwargs):\n",
    "    preds_batch = predict_n(learner, n_imgs, **predict_n_kwargs)\n",
    "    _, axs = plt.subplots(n_imgs, 2, figsize=(6, n_imgs * 3))\n",
    "    for i, (inp, pred_img) in enumerate(preds_batch):\n",
    "        inp.show(ax=axs[i][0])\n",
    "        pred_img.show(ax=axs[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_learner = unet_learner(dls, xresnet18, normalize=True, n_out=n_channels, pretrained=False, \n",
    "                                 loss_func=lambda *args: 0, #cbs=[last_input_cb],\n",
    "                                )\n",
    "generator = generator_learner.model\n",
    "critic = xresnet18(n_out=1)\n",
    "learn = GANLearner.wgan(dls, generator, critic, opt_func = RMSProp)\n",
    "learn.recorder.train_metrics=True\n",
    "learn.recorder.valid_metrics=False\n",
    "learn.fit(1, 2e-4, wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(4, 2e-4, wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(4, 2e-4, wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fid_samples_path = Path('/kaggle/working/fid_samples')\n",
    "n_fid_imgs = 10000\n",
    "\n",
    "def download_pytorch_fid_calculator():        \n",
    "    #!git clone https://github.com/mseitzer/pytorch-fid.git\n",
    "    !pip install pytorch-fid\n",
    "\n",
    "def create_fid_dirs(base_fid_samples_path): \n",
    "    base_fid_samples_path.mkdir()\n",
    "    (base_fid_samples_path/'fake').mkdir()\n",
    "    (base_fid_samples_path/'real').mkdir()\n",
    "\n",
    "def save_real_imgs(dls, n_imgs=10000):\n",
    "    n_imgs_left = n_imgs\n",
    "    while n_imgs_left > 0:\n",
    "        b = dls.one_batch()\n",
    "        bs = b[1].size()[0]\n",
    "        dec_b = dls.decode_batch(b, max_n=bs)\n",
    "        for i in range(bs):\n",
    "            if n_imgs_left == 0: break\n",
    "            target_img_t = dec_b[i][1]\n",
    "            img = PILImage.create(target_img_t)\n",
    "            img_idx = n_imgs_left-1\n",
    "            img.save(base_fid_samples_path/f'real/{img_idx}.jpg')\n",
    "            n_imgs_left -= 1\n",
    "\n",
    "def save_fake_imgs(learner, n_imgs=10000, **predict_n_kwargs):\n",
    "    base_path = base_fid_samples_path\n",
    "    preds_batch = predict_n(learner, n_imgs, **predict_n_kwargs)\n",
    "    for i, (inp, img) in enumerate(preds_batch):\n",
    "        PILImage.create(img).save(base_path/f'fake/{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -R fid_samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fid_dirs(base_fid_samples_path)\n",
    "save_fake_imgs(learn, n_imgs=n_fid_imgs)\n",
    "save_real_imgs(dls, n_fid_imgs)\n",
    "!ls -R fid_samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fid calculator if it isn't yet\n",
    "#download_pytorch_fid_calculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid --device cuda {base_fid_samples_path/'fake'} {base_fid_samples_path/'real'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Once decent results appear, compare them with FID.\n",
    "* UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
    "* Don't forget get_image_files can make training slower at the beginning\n",
    "* Maybe, add cycle consistency loss (this would slow down training), another generator, ... (cycle-GAN)\n",
    "* If no results are gotten:\n",
    "  * use bigger dataset\n",
    "  * use more specific anime faces dataset (it could be worse for production)\n",
    "  * think about initializations\n",
    "* Add/vary transforms\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
