{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from fastai.callback.core import Callback\nfrom fastai.callback.hook import hook_outputs\nfrom fastai.callback.training import ShowGraphCallback\nfrom fastai.vision.augment import Resize, ResizeMethod\nfrom fastai.vision.core import PILImage\nfrom fastai.vision.data import (DataBlock, get_image_files, ImageBlock, ImageDataLoaders, \n                                RandomSplitter)\nfrom fastai.vision.learner import unet_learner\nfrom fastai.vision.models import unet, xresnet\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models.vgg import vgg19\nfrom typing import List, Tuple","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"img_size = (224, 224)\nn_channels = 3\nbs = 8\n# We only need a small subset of the CelebA dataset for this task\nn_ds_items = 100","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_path = Path('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get_image_files is too slow, we can avoid its checks because\n# we know ds_path has a flat structure with only images inside\ndblock = DataBlock(blocks=(ImageBlock, ImageBlock),\n                   #get_items=get_image_files,\n                   get_items=lambda path: path.ls()[:n_ds_items],\n                   splitter=RandomSplitter(0.2),\n                   item_tfms=Resize(img_size[0]))\ndls = dblock.dataloaders(ds_path, path=ds_path, bs=bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeaturesCalculator:\n    def __init__(self, vgg_style_layers_idx:List[int], vgg_content_layers_idx:List[int],\n                 vgg:nn.Module=None, device:torch.device=None):\n        self.vgg = vgg19(pretrained=True) if vgg is None else vgg\n        self.vgg.eval()\n        if device is not None: self.vgg.to(device)\n        modules_to_hook = [self.vgg.features[idx] for idx in (*vgg_style_layers_idx, *vgg_content_layers_idx)]\n        self.hooks = hook_outputs(modules_to_hook, detach=False)\n        self.style_ftrs_hooks = self.hooks[:len(vgg_style_layers_idx)]\n        self.content_ftrs_hooks = self.hooks[len(vgg_style_layers_idx):]\n        # TODO: when to remove hooks? `clean` method????\n    \n    def _get_hooks_out(self, hooks):\n        return [h.stored for h in hooks]\n    \n    def calc_style(self, img_t:torch.Tensor) -> List[torch.Tensor]:\n        self.vgg(img_t)\n        return self._get_hooks_out(self.style_ftrs_hooks)\n    \n    def calc_content(self, img_t:torch.Tensor) -> List[torch.Tensor]:\n        self.vgg(img_t)\n        return self._get_hooks_out(self.content_ftrs_hooks)\n    \n    def calc_style_and_content(self, img_t:torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n        self.vgg(img_t)\n        style_ftrs = self._get_hooks_out(self.style_ftrs_hooks)\n        content_ftrs = self._get_hooks_out(self.content_ftrs_hooks)\n        return style_ftrs, content_ftrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg_content_layers_idx = [22]\nftrs_calc = FeaturesCalculator([], vgg_content_layers_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#n_mid_ftrs = 1000\n#n_out_ftrs = 3\n# encoder = xresnet.xresnet18(p=0.0, c_in=3, n_out=n_mid_ftrs, stem_szs=(32,32,64),\n#                             widen=1.0, sa=False, \n#                             act_cls=defaults.activation, \n#                             ndim=2, ks=3, stride=2, \n#                             #**kwargs # Rest of kwargs go to ConvLayer\n#                            )\n\n# # Remove AvgPool2d. This could be optional\n# encoder[8] = nn.Identity()\n# # Remove final Flatten, Dropout and Linear\n# encoder[9] = nn.Identity()\n# encoder[10] = nn.Identity()\n# encoder[11] = nn.Identity()\n# model = unet.DynamicUnet(encoder, n_out_ftrs, img_size, blur=False, blur_final=True, self_attention=False,\n#                          y_range=None, last_cross=True, bottle=False, \n#                          act_cls=defaults.activation,\n#                          init=nn.init.kaiming_normal_, \n#                          norm_type=NormType.Batch \n#                         )\n\n\n# IT'S BETTER TO USE unet_learner!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_input_cb = Callback()\ncontent_loss = nn.MSELoss(reduction='mean')\n\n\ndef loss_func(output, target):\n    # last_input_cb.x is actually the same as `target` here, but if it wasn't this\n    # would be the std way to access input to measure the preservation of the content\n    input_content_ftrs = ftrs_calc.calc_content(last_input_cb.x)[0]\n    output_content_ftrs = ftrs_calc.calc_content(output)[0]\n    return content_loss(output_content_ftrs, input_content_ftrs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learner"},{"metadata":{"trusted":true},"cell_type":"code","source":"learner = unet_learner(dls, xresnet.xresnet18, normalize=True, n_out=n_channels, pretrained=False,\n                       loss_func=loss_func, cbs=[last_input_cb, ShowGraphCallback()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.show_results()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example of prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_img = dls.valid_ds[0][0]\n_, _, pred_img = learner.predict(sample_img)\npred_img.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}