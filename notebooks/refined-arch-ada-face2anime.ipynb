{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:32.142261Z",
     "iopub.status.busy": "2021-06-05T21:17:32.141923Z",
     "iopub.status.idle": "2021-06-05T21:17:33.942872Z",
     "shell.execute_reply": "2021-06-05T21:17:33.942047Z",
     "shell.execute_reply.started": "2021-06-05T21:17:32.142223Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from functools import partial\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:33.944515Z",
     "iopub.status.busy": "2021-06-05T21:17:33.944114Z",
     "iopub.status.idle": "2021-06-05T21:17:33.952068Z",
     "shell.execute_reply": "2021-06-05T21:17:33.950828Z",
     "shell.execute_reply.started": "2021-06-05T21:17:33.944476Z"
    }
   },
   "outputs": [],
   "source": [
    "run_as_standalone_nb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:33.956464Z",
     "iopub.status.busy": "2021-06-05T21:17:33.956093Z",
     "iopub.status.idle": "2021-06-05T21:17:35.676589Z",
     "shell.execute_reply": "2021-06-05T21:17:35.675697Z",
     "shell.execute_reply.started": "2021-06-05T21:17:33.956425Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_as_standalone_nb:\n",
    "    root_lib_path = Path('face2anime').resolve()\n",
    "    if not root_lib_path.exists():\n",
    "        !git clone https://github.com/davidleonfdez/face2anime.git\n",
    "    if str(root_lib_path) not in sys.path:\n",
    "        sys.path.insert(0, str(root_lib_path))\n",
    "else:\n",
    "    import local_lib_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:35.680687Z",
     "iopub.status.busy": "2021-06-05T21:17:35.680366Z",
     "iopub.status.idle": "2021-06-05T21:17:38.186243Z",
     "shell.execute_reply": "2021-06-05T21:17:38.185469Z",
     "shell.execute_reply.started": "2021-06-05T21:17:35.680654Z"
    }
   },
   "outputs": [],
   "source": [
    "from face2anime.gen_utils import is_iterable\n",
    "from face2anime.layers import ConcatPoolHalfDownsamplingOp2d, ConvHalfDownsamplingOp2d, TransformsLayer\n",
    "from face2anime.losses import ContentLossCallback, CritPredsTracker, LossWrapper, R1GANGPCallback\n",
    "from face2anime.misc import FeaturesCalculator\n",
    "from face2anime.networks import Img2ImgGenerator, patch_res_critic, res_critic\n",
    "from face2anime.train_utils import (add_ema_to_gan_learner, custom_load_model,\n",
    "                                    custom_save_model)\n",
    "from face2anime.transforms import AdaptiveAugmentsCallback, ADATransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:38.189063Z",
     "iopub.status.busy": "2021-06-05T21:17:38.188467Z",
     "iopub.status.idle": "2021-06-05T21:17:38.194147Z",
     "shell.execute_reply": "2021-06-05T21:17:38.193310Z",
     "shell.execute_reply.started": "2021-06-05T21:17:38.189024Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "img_size = 64\n",
    "n_channels = 3\n",
    "bs = 64\n",
    "save_cycle_len = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "animecharacterfaces, by Kaggle user *aadilmalik94*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:38.195965Z",
     "iopub.status.busy": "2021-06-05T21:17:38.195527Z",
     "iopub.status.idle": "2021-06-05T21:17:38.231641Z",
     "shell.execute_reply": "2021-06-05T21:17:38.230849Z",
     "shell.execute_reply.started": "2021-06-05T21:17:38.195926Z"
    }
   },
   "outputs": [],
   "source": [
    "anime_ds_path = Path('/kaggle/input/animecharacterfaces/animeface-character-dataset/data').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:38.233164Z",
     "iopub.status.busy": "2021-06-05T21:17:38.232822Z",
     "iopub.status.idle": "2021-06-05T21:17:42.310722Z",
     "shell.execute_reply": "2021-06-05T21:17:42.309953Z",
     "shell.execute_reply.started": "2021-06-05T21:17:38.233129Z"
    }
   },
   "outputs": [],
   "source": [
    "celeba_path = Path('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba')\n",
    "#input_fns = get_image_files(celeba_path)\n",
    "# get_image_files is too slow, there's no need to check the extension here\n",
    "input_fns = celeba_path.ls()\n",
    "input_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ds path passed to `dblock.dataloaders()` or `ImageDataLoaders.from_dblock()` will be forwarded\n",
    "to `get_items`, which will return a list of items, usually a list of image paths if `get_items=get_image_files`.\n",
    "\n",
    "So, for each item, we are expected to receive a filename `fn` and be able to\n",
    "derive x and y from it, with `get_x(fn)` and `get_y(fn)`.\n",
    "\n",
    "For unpaired image to image translation, we can:\n",
    "* Use the target images ds path as the DataBlock `source`. Then, `get_y` can just return the path received.\n",
    "* Load independently the filenames of the input images ds; let's call it `input_fns`. Then, `get_x` would need to return a random item from `input_fns`. `get_x` is called every time a data item is used; so, by using random, we can be sure every x is not tied to a fixed y; i.e., they won't be together in the same (x, y) batch every epoch for loss calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:42.312563Z",
     "iopub.status.busy": "2021-06-05T21:17:42.312210Z",
     "iopub.status.idle": "2021-06-05T21:17:56.265457Z",
     "shell.execute_reply": "2021-06-05T21:17:56.264680Z",
     "shell.execute_reply.started": "2021-06-05T21:17:42.312525Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_input(fn):\n",
    "    return input_fns[random.randint(0, len(input_fns)-1)]\n",
    "\n",
    "\n",
    "normalize_tf = Normalize.from_stats(torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5]))\n",
    "\n",
    "\n",
    "def get_dblock(extra_batch_tfms=None):\n",
    "    if extra_batch_tfms is None: extra_batch_tfms = []    \n",
    "    return DataBlock(blocks=(ImageBlock, ImageBlock),\n",
    "                     get_x=get_random_input,\n",
    "                     get_items=get_image_files,\n",
    "                     #get_items=lambda path: target_fns,\n",
    "                     splitter=IndexSplitter([]),\n",
    "                     item_tfms=Resize(img_size, method=ResizeMethod.Crop), \n",
    "                     batch_tfms=[normalize_tf] + extra_batch_tfms)\n",
    "\n",
    "\n",
    "dblock = get_dblock()\n",
    "main_path = anime_ds_path\n",
    "dls = dblock.dataloaders(main_path, path=main_path, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:56.267109Z",
     "iopub.status.busy": "2021-06-05T21:17:56.266755Z",
     "iopub.status.idle": "2021-06-05T21:17:58.448564Z",
     "shell.execute_reply": "2021-06-05T21:17:58.447670Z",
     "shell.execute_reply.started": "2021-06-05T21:17:56.267073Z"
    }
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:17:58.450015Z",
     "iopub.status.busy": "2021-06-05T21:17:58.449685Z",
     "iopub.status.idle": "2021-06-05T21:18:20.374931Z",
     "shell.execute_reply": "2021-06-05T21:18:20.374046Z",
     "shell.execute_reply.started": "2021-06-05T21:17:58.449982Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_content_layers_idx = [22]\n",
    "ftrs_calc = FeaturesCalculator([], vgg_content_layers_idx, device=device,\n",
    "                               input_norm_tf=normalize_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T20:23:01.509078Z",
     "iopub.status.busy": "2021-06-04T20:23:01.508357Z",
     "iopub.status.idle": "2021-06-04T20:23:01.562214Z",
     "shell.execute_reply": "2021-06-04T20:23:01.561101Z",
     "shell.execute_reply.started": "2021-06-04T20:23:01.509022Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReconstructionLossWeights:\n",
    "    real_to_real:float=1.\n",
    "    latent_a_to_latent:float=1.\n",
    "    latent_b_to_latent:float=1.\n",
    "\n",
    "class ReconstructionLossCallback(Callback):\n",
    "    def __init__(self, enc_dec_generator:nn.Module, weights:ReconstructionLossWeights,\n",
    "                 n_ch=3, eval_real_b_to_real=True, eval_latent_a_to_latent=True, \n",
    "                 eval_latent_b_to_latent=True, loss_func:Callable=None):\n",
    "        self.generator = enc_dec_generator\n",
    "        self.weights = weights\n",
    "        self.eval_real_b_to_real = eval_real_b_to_real\n",
    "        self.eval_latent_a_to_latent = eval_latent_a_to_latent\n",
    "        self.eval_latent_b_to_latent = eval_latent_b_to_latent\n",
    "        self.latent_layer_idx = 0\n",
    "        if eval_latent_a_to_latent or eval_latent_b_to_latent:\n",
    "            self.latent_layer_idx = self._get_latent_layer_idx(n_ch)\n",
    "        self.loss_func = F.l1_loss if loss_func is None else loss_func\n",
    "        \n",
    "    def _get_latent_layer_idx(self, n_ch:int):\n",
    "        with hook_outputs(self.generator) as all_hooks:\n",
    "            last_spatial_size = 64\n",
    "            self.generator(torch.rand(2, n_ch, last_spatial_size, last_spatial_size))\n",
    "            latent_code_layer_idx = 0\n",
    "            for i, h in enumerate(all_hooks):\n",
    "                if h.stored.shape[-1] > last_spatial_size:\n",
    "                    break\n",
    "                last_spatial_size = h.stored.shape[-1]\n",
    "                latent_code_layer_idx = i\n",
    "            return latent_code_layer_idx\n",
    "        \n",
    "    def after_loss(self):\n",
    "        if not self.gan_trainer.gen_mode: return\n",
    "        eval_any_latent = self.eval_latent_a_to_latent or self.eval_latent_b_to_latent\n",
    "        mid_hook = hook_output(self.generator[self.latent_layer_idx]) if eval_any_latent else None\n",
    "        #encoder = nn.Sequential(*self.generator[:self.latent_layer_idx+1]) if eval_any_latent else None\n",
    "        \n",
    "        if self.eval_real_b_to_real:\n",
    "            fake = self.generator(self.y)\n",
    "            real_to_real_loss = self.loss_func(fake, self.y) * self.weights.real_to_real\n",
    "            self.learn.loss_grad += real_to_real_loss\n",
    "            # Store result inside learn.loss_func to make it visible to metrics display\n",
    "            self.learn.loss_func.real_rec_loss = real_to_real_loss\n",
    "            \n",
    "        # Watchout: order matters! This `if` needs to be placed before the next one\n",
    "        # (`if self.eval_latent_a_to_latent`) because mid_hook.stored is reused \n",
    "        # when (self.eval_real_b_to_real == True)       \n",
    "        if self.eval_latent_b_to_latent:\n",
    "            if mid_hook.stored is None:\n",
    "                fake = self.generator(self.y)\n",
    "            latent_b = mid_hook.stored\n",
    "            # TODO: it would be more efficient to execute `encoder(fake)` only, but not sure \n",
    "            # if it could be possible w/o losing computation graph\n",
    "            self.generator(fake)\n",
    "            latent_b_rec = mid_hook.stored\n",
    "            latent_b_rec_loss = self.loss_func(latent_b, latent_b_rec) * self.weights.latent_b_to_latent\n",
    "            self.learn.loss_grad += latent_b_rec_loss\n",
    "            # Store result inside learn.loss_func to make it visible to metrics display\n",
    "            self.learn.loss_func.latent_b_rec_loss = latent_b_rec_loss\n",
    "            \n",
    "        if self.eval_latent_a_to_latent:\n",
    "            fake = self.generator(self.x)\n",
    "            latent_a = mid_hook.stored\n",
    "            self.generator(fake)\n",
    "            latent_a_rec = mid_hook.stored\n",
    "            latent_a_rec_loss = self.loss_func(latent_a, latent_a_rec) * self.weights.latent_a_to_latent\n",
    "            self.learn.loss_grad += latent_a_rec_loss\n",
    "            # Store result inside learn.loss_func to make it visible to metrics display\n",
    "            self.learn.loss_func.latent_a_rec_loss = latent_a_rec_loss\n",
    "            \n",
    "        if eval_any_latent:\n",
    "            mid_hook.remove()\n",
    "\n",
    "\n",
    "class DummyGen(nn.Module):\n",
    "    \"\"\"Generator composed by convs whose weights are full of ones with no bias.\"\"\"\n",
    "    def __init__(self, n_ftrs, k_szs, strides, paddings, transpose):\n",
    "        super().__init__()\n",
    "        convs = []\n",
    "        for i, (nf, ks, stride, pad, tr) in enumerate(zip(n_ftrs[:-1], k_szs, strides, paddings, transpose)):\n",
    "            conv_func = nn.ConvTranspose2d if tr else nn.Conv2d\n",
    "            conv = conv_func(nf, n_ftrs[i+1], kernel_size=ks, stride=stride, padding=pad, bias=False)\n",
    "            convs.append(conv)\n",
    "            nn.init.constant_(conv.weight, 1)\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "    def __iter__(self): return iter(self.convs)\n",
    "    def __getitem__(self,i): return self.convs[i]\n",
    "\n",
    "class DummyCritic(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x): return torch.zeros(x.size()[0], 1).requires_grad_(True)\n",
    "    \n",
    "\n",
    "def test_rec_loss():        \n",
    "    n_ch = 3\n",
    "    mid_ftrs = 6\n",
    "    resample_ks = 4\n",
    "    dblock = DataBlock(blocks=(ImageBlock, ImageBlock),\n",
    "                       # float tensor full of 1.'s\n",
    "                       get_x=lambda fn: np.full((4, 4, n_ch), 255, dtype=np.uint8),\n",
    "                       splitter = IndexSplitter([]),\n",
    "                       # float tensor full of 0.2's\n",
    "                       get_y=lambda fn: np.full((4, 4, n_ch), 51, dtype=np.uint8))\n",
    "    dls = dblock.dataloaders(['', ''], bs=2)\n",
    "    gen = DummyGen([n_ch, mid_ftrs, n_ch], [resample_ks, resample_ks], \n",
    "                   [2, 2], [0, 0], [False, True])\n",
    "    crit = DummyCritic()    \n",
    "    weights = ReconstructionLossWeights(1, 2, 3)\n",
    "    rec_loss_cb = ReconstructionLossCallback(gen, weights)\n",
    "    learn = GANLearner.wgan(dls, gen, crit, cbs=[rec_loss_cb], gen_first=True) #metrics = [...]\n",
    "    learn.fit(1)\n",
    "    \n",
    "    deeper_gen = DummyGen([n_ch, mid_ftrs, mid_ftrs, mid_ftrs, n_ch], [resample_ks, 3, 3, resample_ks], \n",
    "                          [2, 1, 1, 2], [0, 0, 0, 0], [False, False, False, True])\n",
    "    rec_loss_cb_deeper_gen = ReconstructionLossCallback(deeper_gen, weights)\n",
    "    \n",
    "    \n",
    "    # Expected results are almost hardcoded in order to avoid repeating potential coding errors\n",
    "    # from test code, although we are unnecessarily testing DummyGen at the same time.\n",
    "    # If input=torch.full((bs, n_ch, 4, 4), item_val) ...\n",
    "    #   -After first conv, out=torch.full((bs, 6, 1, 1), item_val*n_ch*(resample_ks**2))\n",
    "    #   -After second conv, out=torch.full((bs, 3, 4, 4), (item_val*n_ch*(resample_ks**2))*mid_ftrs\n",
    "    #   -After first conv, second forward, out=torch.full((bs, 6, 1, 1), ((item_val*n_ch*(resample_ks**2))*mid_ftrs)*n_ch*(resample_ks**2))\n",
    "    real_out_values = 0.2 * mid_ftrs * n_ch * resample_ks**2\n",
    "    expected_real_rec_loss = weights.real_to_real * abs(real_out_values - 0.2)    \n",
    "    \n",
    "    # Latent code obtained passing real target (y) as input\n",
    "    latent_b_values = 0.2 * n_ch * resample_ks**2\n",
    "    latent_b_rec_values = 0.2 * mid_ftrs * n_ch**2 * resample_ks**4\n",
    "    expected_latent_b_rec_loss = weights.latent_b_to_latent * abs(latent_b_rec_values - latent_b_values)\n",
    "    \n",
    "    # Latent code obtained passing real input (x) as input\n",
    "    latent_a_values = n_ch * resample_ks**2\n",
    "    latent_a_rec_values = mid_ftrs * n_ch**2 * resample_ks**4\n",
    "    expected_latent_a_rec_loss = weights.latent_a_to_latent * abs(latent_a_rec_values - latent_a_values)\n",
    "\n",
    "    # A more concise but also error prone form would be:\n",
    "#     y = torch.full((bs, n_ch, 4, 4), 0.2)\n",
    "#     real_out = gen(y)\n",
    "#     expected_real_rec_loss = weights.real_to_real * F.l1_loss(real_out, y)\n",
    "#     latent_b = gen[0](y)\n",
    "#     latent_b_rec = gen[0](gen[1](latent_b)) \n",
    "#     expected_latent_b_rec_loss = weights.latent_b_to_latent * F.l1_loss(latent_b_rec, latent_b)\n",
    "#     x = torch.ones(bs, n_ch, 4, 4)\n",
    "#     latent_a = gen[0](x)\n",
    "#     latent_a_rec = gen[0](gen[1](latent_a))\n",
    "#     expected_latent_a_rec_loss = weights.latent_a_to_latent * F.l1_loss(latent_a_rec, latent_a)\n",
    "    \n",
    "    expected_loss = expected_real_rec_loss + expected_latent_b_rec_loss + expected_latent_a_rec_loss\n",
    "        \n",
    "    assert rec_loss_cb._get_latent_layer_idx(n_ch) == 0\n",
    "    assert rec_loss_cb_deeper_gen._get_latent_layer_idx(n_ch) == 2\n",
    "    assert math.isclose(learn.recorder.losses[0], expected_loss, rel_tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T20:23:01.564217Z",
     "iopub.status.busy": "2021-06-04T20:23:01.563658Z",
     "iopub.status.idle": "2021-06-04T20:23:01.98965Z",
     "shell.execute_reply": "2021-06-04T20:23:01.988303Z",
     "shell.execute_reply.started": "2021-06-04T20:23:01.564169Z"
    }
   },
   "outputs": [],
   "source": [
    "test_rec_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:18:20.376608Z",
     "iopub.status.busy": "2021-06-05T21:18:20.376221Z",
     "iopub.status.idle": "2021-06-05T21:18:20.392478Z",
     "shell.execute_reply": "2021-06-05T21:18:20.391625Z",
     "shell.execute_reply.started": "2021-06-05T21:18:20.376557Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_n(learner, n_imgs, max_bs=64):\n",
    "    dummy_path = Path('')\n",
    "    dl = learner.dls.test_dl([dummy_path]*n_imgs, bs=max_bs)   \n",
    "    inp, imgs_t, _, dec_imgs_t = learner.get_preds(dl=dl, with_input=True, with_decoded=True)\n",
    "    dec_batch = dls.decode_batch((inp,) + tuplify(dec_imgs_t), max_n=n_imgs)\n",
    "    return dec_batch\n",
    "    \n",
    "def predict_show_n(learner, n_imgs, **predict_n_kwargs):\n",
    "    preds_batch = predict_n(learner, n_imgs, **predict_n_kwargs)\n",
    "    _, axs = plt.subplots(n_imgs, 2, figsize=(6, n_imgs * 3))\n",
    "    for i, (inp, pred_img) in enumerate(preds_batch):\n",
    "        inp.show(ax=axs[i][0])\n",
    "        pred_img.show(ax=axs[i][1])\n",
    "        \n",
    "class SaveCheckpointsCallback(Callback):\n",
    "    \"Callback that saves the model at the end of each epoch.\"\n",
    "    def __init__(self, learn, fn_prefix, base_path=Path('.'), initial_epoch=1,\n",
    "                 save_cycle_len=1):\n",
    "        self.fn_prefix = fn_prefix\n",
    "        self.base_path = base_path\n",
    "        self.epoch = initial_epoch\n",
    "        self.save_cycle_len = save_cycle_len\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        if (self.epoch % self.save_cycle_len) == 0:\n",
    "            fn = f'{self.fn_prefix}_{self.epoch}ep'\n",
    "            custom_save_model(learn, fn, base_path=self.base_path)\n",
    "        self.epoch += 1\n",
    "        \n",
    "def save_preds(c_preds_tracker, filepath):\n",
    "    return c_preds_tracker.to_df().to_csv(filepath)\n",
    "        \n",
    "def plot_c_preds(c_preds_tracker):\n",
    "    preds_xs = range(len(c_preds_tracker.real_preds))\n",
    "    sns.lineplot(x=preds_xs, y=c_preds_tracker.fake_preds.cpu(), label='Fake preds')\n",
    "    ax=sns.lineplot(x=preds_xs, y=c_preds_tracker.real_preds.cpu(), label='Real preds')\n",
    "    ax.set_xlabel('Number of batches')\n",
    "    ax.set_ylabel('Critic preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:18:20.394416Z",
     "iopub.status.busy": "2021-06-05T21:18:20.393887Z",
     "iopub.status.idle": "2021-06-05T21:18:20.406890Z",
     "shell.execute_reply": "2021-06-05T21:18:20.406060Z",
     "shell.execute_reply.started": "2021-06-05T21:18:20.394372Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_inn_options(net, **inn_kwargs):\n",
    "    for k, module in net.named_modules():\n",
    "        if not isinstance(module, nn.InstanceNorm2d): continue\n",
    "        editable_module = net\n",
    "        accesors = k.split('.')\n",
    "        for accesor in accesors[:-1]:\n",
    "            editable_module = (editable_module[int(accesor)] if accesor.isnumeric()\n",
    "                              else getattr(editable_module, accesor))\n",
    "        new_module = InstanceNorm(module.num_features, **inn_kwargs)\n",
    "        if accesors[-1].isnumeric():\n",
    "            editable_module[int(accesors[-1])] = new_module\n",
    "        else:\n",
    "            setattr(editable_module, accesors[-1], new_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:18:20.408800Z",
     "iopub.status.busy": "2021-06-05T21:18:20.408258Z",
     "iopub.status.idle": "2021-06-05T21:18:20.429451Z",
     "shell.execute_reply": "2021-06-05T21:18:20.428556Z",
     "shell.execute_reply.started": "2021-06-05T21:18:20.408756Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ADAConfig:\n",
    "    p_change_thresh:float=0.6\n",
    "    filter_tfms_to_array:Callable=None\n",
    "    pad_mode:PadMode=PadMode.Reflection\n",
    "    \n",
    "\n",
    "def create_learner(for_inference=False, ada_conf=None, dblock=dblock, dls=dls, gp_w=1.,\n",
    "                   latent_sz=100, g_norm=NormType.Instance, mid_mlp_depth=0,\n",
    "                   n_extra_convs_by_c_res_block=0, g_skips=False, n_crit_iters=1, \n",
    "                   metrics=None, use_patch_critic=False):\n",
    "    use_ada = ada_conf is not None\n",
    "    leakyReLU02 = partial(nn.LeakyReLU, negative_slope=0.2)\n",
    "    down_op = ConvHalfDownsamplingOp2d(ks=4, act_cls=leakyReLU02, bn_1st=False,\n",
    "                                       norm_type=NormType.Batch)\n",
    "    id_down_op = ConcatPoolHalfDownsamplingOp2d(conv_ks=3, act_cls=None, norm_type=None)\n",
    "    crit_args = [img_size, n_channels, down_op, id_down_op]\n",
    "    if use_patch_critic: crit_args.insert(2, img_size//8)\n",
    "    crit_kwargs = dict(n_extra_convs_by_res_block=n_extra_convs_by_c_res_block, \n",
    "                       act_cls=leakyReLU02, bn_1st=False, n_features=128, \n",
    "                       flatten_full=True)\n",
    "    crit_builder = patch_res_critic if use_patch_critic else res_critic\n",
    "    base_critic = crit_builder(*crit_args, **crit_kwargs)\n",
    "    if not use_ada: critic = base_critic\n",
    "    \n",
    "    def _decoder_builder(imsz, nch, latsz, hooks_by_sz=None): \n",
    "        return default_decoder(imsz, nch, latsz, norm_type=g_norm, hooks_by_sz=hooks_by_sz)\n",
    "    generator = Img2ImgGenerator(img_size, n_channels, mid_mlp_depth=mid_mlp_depth, skip_connect=g_skips,\n",
    "                                 encoder=default_encoder(img_size, n_channels, latent_sz, norm_type=g_norm),\n",
    "                                 decoder_builder=_decoder_builder)\n",
    "    \n",
    "    cbs = []\n",
    "    c_loss_interceptors = []\n",
    "    tfms_array = []\n",
    "    \n",
    "    if not for_inference:\n",
    "        # Pass base_critic to avoid grid_sample 2nd order derivative issue with ADA critic\n",
    "        cbs.append(R1GANGPCallback(weight=gp_w, critic=base_critic))\n",
    "        #cbs.append(ContentLossCallback(weight=content_loss_w, ftrs_calc=ftrs_calc, device=device))\n",
    "        #cbs.append(ReconstructionLossCallback(generator, rec_loss_weights, **rec_loss_cb_kwargs))\n",
    "        if use_ada:\n",
    "            ada_tfms = ADATransforms(0., (img_size, img_size), pad_mode=ada_conf.pad_mode)\n",
    "            tfms_array = (ada_conf.filter_tfms_to_array(ada_tfms) if ada_conf.filter_tfms_to_array is not None\n",
    "                          else ada_tfms.to_array())\n",
    "            ada_crit_preds_tracker = CritPredsTracker(reduce_batch=False)\n",
    "            ada_cb = AdaptiveAugmentsCallback(ada_tfms, ada_crit_preds_tracker,\n",
    "                                              preds_above_0_overfit_threshold=ada_conf.p_change_thresh)\n",
    "            cbs.append(ada_cb)\n",
    "            c_loss_interceptors.append(ada_crit_preds_tracker)\n",
    "        overall_crit_preds_tracker = CritPredsTracker(reduce_batch=True)\n",
    "        c_loss_interceptors.append(overall_crit_preds_tracker)\n",
    "       \n",
    "    if use_ada:\n",
    "        critic = nn.Sequential(TransformsLayer(setup_aug_tfms(tfms_array)),\n",
    "                               base_critic)       \n",
    "    \n",
    "    def gen_loss_func(*args): return 0\n",
    "    crit_loss_func = nn.BCEWithLogitsLoss()\n",
    "    loss_G, loss_C = gan_loss_from_func(gen_loss_func, crit_loss_func)\n",
    "    loss_C = LossWrapper(loss_C, c_loss_interceptors)\n",
    "    \n",
    "    learn = GANLearner(dls, generator, critic, loss_G, loss_C,\n",
    "                       opt_func=partial(Adam, mom=0., sqr_mom=0.99, wd=0.),\n",
    "                       cbs=cbs, switcher=FixedGANSwitcher(n_crit=n_crit_iters, n_gen=1),\n",
    "                       switch_eval=False, metrics=metrics)\n",
    "    #metrics=LossMetric('content_loss')\n",
    "    #metrics=LossMetrics(['real_rec_loss', 'latent_a_rec_loss', 'latent_b_rec_loss'])\n",
    "    learn.recorder.train_metrics=True\n",
    "    learn.recorder.valid_metrics=False\n",
    "    add_ema_to_gan_learner(learn, dblock, main_path, decay=0.999)\n",
    "    if not for_inference: learn.crit_preds_tracker = overall_crit_preds_tracker\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR 1: NSGAN-R1GP loss, SN+BN critic, IN+SN gen (both encoder and decoder), no mid MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TR 1a: global critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:18:20.447061Z",
     "iopub.status.busy": "2021-06-05T21:18:20.446522Z",
     "iopub.status.idle": "2021-06-05T21:18:20.458376Z",
     "shell.execute_reply": "2021-06-05T21:18:20.457537Z",
     "shell.execute_reply.started": "2021-06-05T21:18:20.447023Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_learner_1(*args, **kwargs):\n",
    "    return create_learner(*args, **kwargs, gp_w=10., g_norm=NormType.Batch, mid_mlp_depth=2,\n",
    "                          g_skips=True, n_extra_convs_by_c_res_block=1, n_crit_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:18:20.459982Z",
     "iopub.status.busy": "2021-06-05T21:18:20.459473Z",
     "iopub.status.idle": "2021-06-05T21:18:21.636005Z",
     "shell.execute_reply": "2021-06-05T21:18:21.635263Z",
     "shell.execute_reply.started": "2021-06-05T21:18:20.459937Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = create_learner_1()\n",
    "learn.add_cb(SaveCheckpointsCallback(learn, 'refined_arch_face2anime_tr1', initial_epoch=1,\n",
    "                                     save_cycle_len=save_cycle_len))\n",
    "ema_g_learn = Learner(dls, learn.ema_model, loss_func=lambda *args: torch.tensor(0.))\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T20:51:35.323456Z",
     "iopub.status.busy": "2021-06-05T20:51:35.323043Z",
     "iopub.status.idle": "2021-06-05T20:51:54.280088Z",
     "shell.execute_reply": "2021-06-05T20:51:54.278942Z",
     "shell.execute_reply.started": "2021-06-05T20:51:35.323424Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# custom_load_model(learn, 'refined_arch_face2anime_tr1_100ep', base_path='../input/refined-arch-face2anime/', with_ema=True)\n",
    "# #preds_df = pd.read_csv(io.StringIO(preds_csv_str), index_col=0)\n",
    "# #learn.crit_preds_tracker.load_from_df(preds_df, device)\n",
    "# with learn.removed_cbs([learn.save_checkpoints]) as displayable_learn:\n",
    "#     displayable_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T21:18:34.197054Z",
     "iopub.status.busy": "2021-06-05T21:18:34.196728Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit(100, lr)\n",
    "with learn.removed_cbs([learn.save_checkpoints]) as displayable_learn:\n",
    "    displayable_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T02:36:38.610347Z",
     "iopub.status.busy": "2021-06-05T02:36:38.609973Z",
     "iopub.status.idle": "2021-06-05T02:36:40.721681Z",
     "shell.execute_reply": "2021-06-05T02:36:40.720775Z",
     "shell.execute_reply.started": "2021-06-05T02:36:38.610302Z"
    }
   },
   "outputs": [],
   "source": [
    "ema_g_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T02:36:40.723858Z",
     "iopub.status.busy": "2021-06-05T02:36:40.723441Z",
     "iopub.status.idle": "2021-06-05T02:36:41.907424Z",
     "shell.execute_reply": "2021-06-05T02:36:41.9067Z",
     "shell.execute_reply.started": "2021-06-05T02:36:40.723813Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_c_preds(learn.crit_preds_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T02:37:12.040254Z",
     "iopub.status.busy": "2021-06-04T02:37:12.039895Z",
     "iopub.status.idle": "2021-06-04T02:37:12.08461Z",
     "shell.execute_reply": "2021-06-04T02:37:12.083893Z",
     "shell.execute_reply.started": "2021-06-04T02:37:12.040214Z"
    }
   },
   "outputs": [],
   "source": [
    "save_preds(learn.crit_preds_tracker, Path('crit_preds_face2anime_refined_tr1_100ep.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TR 1b: patch critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner_1b(*args, **kwargs):\n",
    "    return create_learner(*args, **kwargs, gp_w=10., g_norm=NormType.Batch, mid_mlp_depth=2,\n",
    "                          g_skips=True, n_extra_convs_by_c_res_block=0, n_crit_iters=3,\n",
    "                          use_patch_critic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner_1b()\n",
    "learn.add_cb(SaveCheckpointsCallback(learn, 'refined_arch_face2anime_tr1b', initial_epoch=1,\n",
    "                                     save_cycle_len=save_cycle_len))\n",
    "ema_g_learn = Learner(dls, learn.ema_model, loss_func=lambda *args: torch.tensor(0.))\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(100, lr)\n",
    "with learn.removed_cbs([learn.save_checkpoints]) as displayable_learn:\n",
    "    displayable_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_g_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_c_preds(learn.crit_preds_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(learn.crit_preds_tracker, Path('crit_preds_face2anime_refined_tr1b_100ep.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR2 [TR1 + ADA]: NSGAN-R1GP loss, SN+BN critic, IN+SN gen (both encoder and decoder), no mid MLP, ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T02:44:46.520089Z",
     "iopub.status.busy": "2021-06-04T02:44:46.519737Z",
     "iopub.status.idle": "2021-06-04T02:44:46.715835Z",
     "shell.execute_reply": "2021-06-04T02:44:46.715094Z",
     "shell.execute_reply.started": "2021-06-04T02:44:46.520058Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_spatial_minus_flip_rot_small(ada_tfms):\n",
    "    return [tfm for tfm in ada_tfms.to_array() \n",
    "            if (isinstance(tfm, (AffineCoordTfm)) and not isinstance(tfm, (Rotate, Flip)))\n",
    "                or tfm == ada_tfms.rotate_90x]\n",
    "\n",
    "aug_dblock = get_dblock(extra_batch_tfms=[Flip(0.5)])\n",
    "aug_dls = aug_dblock.dataloaders(main_path, path=main_path, bs=bs)\n",
    "\n",
    "\n",
    "def create_learner_2(*args, **kwargs):\n",
    "    return create_learner(*args, \n",
    "                          ada_conf=ADAConfig(filter_tfms_to_array=filter_spatial_minus_flip_rot_small, \n",
    "                                             p_change_thresh=0.8), \n",
    "                          dblock=aug_dblock,\n",
    "                          dls=aug_dls,\n",
    "                          gp_w=10., \n",
    "                          g_norm=NormType.Batch, \n",
    "                          mid_mlp_depth=2,\n",
    "                          g_skips=True, \n",
    "                          n_extra_convs_by_c_res_block=1, \n",
    "                          n_crit_iters=3,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TR 2a: global critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T02:44:47.634565Z",
     "iopub.status.busy": "2021-06-04T02:44:47.634214Z",
     "iopub.status.idle": "2021-06-04T02:44:48.557208Z",
     "shell.execute_reply": "2021-06-04T02:44:48.556453Z",
     "shell.execute_reply.started": "2021-06-04T02:44:47.634533Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = create_learner_2()\n",
    "learn.add_cb(SaveCheckpointsCallback(learn, 'refined_arch_face2anime_tr2', initial_epoch=1,\n",
    "                                     save_cycle_len=save_cycle_len))\n",
    "ema_g_learn = Learner(dls, learn.ema_model, loss_func=lambda *args: torch.tensor(0.))\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-04T02:55:32.555078Z",
     "iopub.status.idle": "2021-06-04T02:55:32.555925Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit(100, lr)\n",
    "with learn.removed_cbs([learn.save_checkpoints, learn.loss_store]) as displayable_learn:\n",
    "    displayable_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-04T02:55:32.548756Z",
     "iopub.status.idle": "2021-06-04T02:55:32.549502Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_c_preds(learn.crit_preds_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-04T02:55:32.550843Z",
     "iopub.status.idle": "2021-06-04T02:55:32.55166Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(len(learn.adaptive_augments.p_history)), y=learn.adaptive_augments.p_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(learn.crit_preds_tracker, Path('crit_preds_face2anime_refined_tr2_100ep.csv'))\n",
    "!echo {learn.adaptive_augments.p_history} > p_history_face2anime_refined_tr2_100ep.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TR 2b: patch critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner_2(*args, **kwargs):\n",
    "    return create_learner(*args, \n",
    "                          ada_conf=ADAConfig(filter_tfms_to_array=filter_spatial_minus_flip_rot_small, \n",
    "                                             p_change_thresh=0.8), \n",
    "                          dblock=aug_dblock,\n",
    "                          dls=aug_dls,\n",
    "                          gp_w=10., \n",
    "                          g_norm=NormType.Batch, \n",
    "                          mid_mlp_depth=2,\n",
    "                          g_skips=True, \n",
    "                          n_extra_convs_by_c_res_block=1, \n",
    "                          n_crit_iters=3,\n",
    "                          use_patch_critic=True,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner_2b()\n",
    "learn.add_cb(SaveCheckpointsCallback(learn, 'refined_arch_face2anime_tr2b', initial_epoch=1,\n",
    "                                     save_cycle_len=save_cycle_len))\n",
    "ema_g_learn = Learner(dls, learn.ema_model, loss_func=lambda *args: torch.tensor(0.))\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(100, lr)\n",
    "with learn.removed_cbs([learn.save_checkpoints, learn.loss_store]) as displayable_learn:\n",
    "    displayable_learn.show_results(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_c_preds(learn.crit_preds_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(len(learn.adaptive_augments.p_history)), y=learn.adaptive_augments.p_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(learn.crit_preds_tracker, Path('crit_preds_face2anime_refined_tr2b_100ep.csv'))\n",
    "!echo {learn.adaptive_augments.p_history} > p_history_face2anime_refined_tr2b_100ep.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T00:22:01.670733Z",
     "iopub.status.busy": "2021-06-05T00:22:01.670399Z",
     "iopub.status.idle": "2021-06-05T00:22:01.687959Z",
     "shell.execute_reply": "2021-06-05T00:22:01.686892Z",
     "shell.execute_reply.started": "2021-06-05T00:22:01.670704Z"
    }
   },
   "outputs": [],
   "source": [
    "base_fid_samples_path = Path('/kaggle/working/fid_samples')\n",
    "n_fid_imgs = 10000\n",
    "\n",
    "def download_pytorch_fid_calculator():        \n",
    "    #!git clone https://github.com/mseitzer/pytorch-fid.git\n",
    "    !pip install pytorch-fid\n",
    "\n",
    "def create_fid_dirs(base_fid_samples_path):\n",
    "    base_fid_samples_path.mkdir()\n",
    "    (base_fid_samples_path/'fake').mkdir()\n",
    "    (base_fid_samples_path/'real').mkdir()\n",
    "        \n",
    "def save_real_imgs(dls, n_imgs=10000, use_input_ds=False):\n",
    "    n_imgs_left = n_imgs\n",
    "    while n_imgs_left > 0:\n",
    "        b = dls.one_batch()\n",
    "        bs = b[1].size()[0]\n",
    "        dec_b = dls.decode_batch(b, max_n=bs)\n",
    "        for i in range(bs):\n",
    "            if n_imgs_left == 0: break\n",
    "            tuple_idx = 0 if use_input_ds else 1\n",
    "            img_t = dec_b[i][tuple_idx]\n",
    "            img = PILImage.create(img_t)\n",
    "            img_idx = n_imgs_left-1\n",
    "            img.save(base_fid_samples_path/f'real/{img_idx}.jpg')\n",
    "            #if n_imgs_left % 1000 == 0: print(\"saved \" + str(img_idx))\n",
    "            n_imgs_left -= 1            \n",
    "            \n",
    "def save_fake_imgs(learner, n_imgs=10000, **predict_n_kwargs):\n",
    "    base_path = base_fid_samples_path\n",
    "    preds_batch = predict_n(learner, n_imgs, **predict_n_kwargs)\n",
    "    for i, (inp, img) in enumerate(preds_batch):\n",
    "        PILImage.create(img).save(base_path/f'fake/{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!rm -R $base_fid_samples_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T00:22:02.867673Z",
     "iopub.status.busy": "2021-06-05T00:22:02.867354Z",
     "iopub.status.idle": "2021-06-05T00:22:11.096421Z",
     "shell.execute_reply": "2021-06-05T00:22:11.095552Z",
     "shell.execute_reply.started": "2021-06-05T00:22:02.867644Z"
    }
   },
   "outputs": [],
   "source": [
    "download_pytorch_fid_calculator()\n",
    "create_fid_dirs(base_fid_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T00:22:11.098744Z",
     "iopub.status.busy": "2021-06-05T00:22:11.098366Z",
     "iopub.status.idle": "2021-06-05T00:22:11.115715Z",
     "shell.execute_reply": "2021-06-05T00:22:11.114792Z",
     "shell.execute_reply.started": "2021-06-05T00:22:11.098703Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_models(builders, n_epochs, base_path='/kaggle/input/new-face2anime-weights', ema=False):\n",
    "    assert is_iterable(builders) or is_iterable(n_epochs)\n",
    "    if not is_iterable(builders): \n",
    "        builders = [builders] * len(list(n_epochs))\n",
    "    if not is_iterable(n_epochs): \n",
    "        n_epochs = [n_epochs] * len(list(builders))\n",
    "    for builder, n_ep in zip(builders, n_epochs):\n",
    "        model_id = builder.__name__.split('_')[-1]\n",
    "        learner = builder(for_inference=True)\n",
    "        custom_load_model(learner, f'refined_arch_face2anime_tr{model_id}_{n_ep}ep', with_opt=False,\n",
    "                          base_path=base_path)\n",
    "        if ema: \n",
    "            learner = Learner(learner.dls, learner.ema_model,\n",
    "                              loss_func=lambda *args: torch.tensor(0.))\n",
    "        save_fake_imgs(learner, n_imgs=n_fid_imgs)\n",
    "        print(f'---- {model_id}, after {n_ep} epochs ----')\n",
    "        !python -m pytorch_fid {base_fid_samples_path/'fake'} {base_fid_samples_path/'real'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T00:22:19.962962Z",
     "iopub.status.busy": "2021-06-05T00:22:19.962619Z",
     "iopub.status.idle": "2021-06-05T00:24:51.917212Z",
     "shell.execute_reply": "2021-06-05T00:24:51.916285Z",
     "shell.execute_reply.started": "2021-06-05T00:22:19.96293Z"
    }
   },
   "outputs": [],
   "source": [
    "save_real_imgs(dls, n_fid_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T01:34:22.529585Z",
     "iopub.status.busy": "2021-06-05T01:34:22.529245Z",
     "iopub.status.idle": "2021-06-05T01:57:39.555605Z",
     "shell.execute_reply": "2021-06-05T01:57:39.55467Z",
     "shell.execute_reply.started": "2021-06-05T01:34:22.529551Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_models(create_learner_1, range(5, 101, 5), base_path='./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T02:51:55.033254Z",
     "iopub.status.busy": "2021-06-05T02:51:55.032886Z",
     "iopub.status.idle": "2021-06-05T03:07:41.985472Z",
     "shell.execute_reply": "2021-06-05T03:07:41.984519Z",
     "shell.execute_reply.started": "2021-06-05T02:51:55.033213Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_models(create_learner_1, range(5, 101, 5), base_path='./models', ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T01:57:39.560081Z",
     "iopub.status.busy": "2021-06-05T01:57:39.559797Z",
     "iopub.status.idle": "2021-06-05T01:57:39.571858Z",
     "shell.execute_reply": "2021-06-05T01:57:39.570849Z",
     "shell.execute_reply.started": "2021-06-05T01:57:39.56005Z"
    }
   },
   "outputs": [],
   "source": [
    "PILImage.create(base_fid_samples_path/'fake/9999.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10000 images:\n",
    "\n",
    "* FID input ds vs itself (CelebA vs CelebA) ~ 2.8\n",
    "* FID target ds vs itself (Animecharacterfaces vs Animecharacterfaces) ~ 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
